{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797c27d8",
   "metadata": {},
   "source": [
    "# ğŸ“Š Pipelines V4 Production - Architecture\n",
    "\n",
    "Ce notebook dÃ©finit **8 pipelines individuels** et **4 pipelines nested** pour le projet COVID-19.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Architecture HiÃ©rarchique des Transformateurs\n",
    "\n",
    "```\n",
    "ğŸ“¦ TRANSFORMATEURS COMMUNS (utilisÃ©s par tous)\n",
    "â”‚\n",
    "â”œâ”€ ğŸ” ImagePathLoader          â†’ Scan directories et rÃ©cupÃ¨re chemins\n",
    "â”œâ”€ ğŸ“‹ TupleToDataFrame          â†’ Convertit tuples en DataFrame\n",
    "â””â”€ ğŸ“Š ImageAnalyser             â†’ Load images + analyse mÃ©tadonnÃ©es\n",
    "\n",
    "ğŸ“¦ PREPROCESSING (selon type de modÃ¨le)\n",
    "â”‚\n",
    "â”œâ”€ ğŸ–¼ï¸  ML Classique (64x64, grayscale)\n",
    "â”‚   â”œâ”€ ImageResizer (64x64)\n",
    "â”‚   â”œâ”€ ImageNormalizer [0,1]\n",
    "â”‚   â””â”€ RGB_to_L (grayscale)\n",
    "â”‚\n",
    "â”œâ”€ ğŸ§  CNN Custom (128x128, RGB)\n",
    "â”‚   â”œâ”€ ImageResizer (128x128)\n",
    "â”‚   â””â”€ ImageNormalizer [0,1]\n",
    "â”‚\n",
    "â””â”€ ğŸ”„ Transfer Learning (224x224, RGB)\n",
    "    â”œâ”€ ImageResizer (224x224)\n",
    "    â””â”€ ImageNormalizer [0,1]\n",
    "\n",
    "ğŸ“¦ FEATURES EXTRACTION (pour ML uniquement)\n",
    "â”‚\n",
    "â”œâ”€ ğŸ“ PCA Features\n",
    "â”‚   â”œâ”€ ImageFlattener\n",
    "â”‚   â”œâ”€ ImagePCA (50 components)\n",
    "â”‚   â””â”€ PCAVisualizer (NEW)\n",
    "â”‚\n",
    "â””â”€ ğŸ“Š Histogram Features\n",
    "    â”œâ”€ ImageHistogram (32 bins)\n",
    "    â””â”€ HistogramVisualizer (NEW)\n",
    "\n",
    "ğŸ“¦ AUGMENTATION (optionnel)\n",
    "â”‚\n",
    "â””â”€ ğŸ¨ Data Augmentation\n",
    "    â”œâ”€ ImageAugmenter (rotation, brightness, noise, zoom)\n",
    "    â”œâ”€ ImageMasker (optionnel)\n",
    "    â””â”€ ImageComparisonVisualizer (NEW)\n",
    "\n",
    "ğŸ“¦ UTILITIES\n",
    "â”‚\n",
    "â”œâ”€ ğŸ¯ TrainTestSplitter         â†’ Split stratifiÃ©\n",
    "â”œâ”€ ğŸ“ˆ DatasetStatistics (NEW)   â†’ Stats + visualisations\n",
    "â””â”€ ğŸ’¾ SaveTransformer           â†’ Sauvegarde intermÃ©diaire\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Matrice d'Utilisation des Pipelines\n",
    "\n",
    "| Transformateur | Pipeline 1<br/>Exploration | Pipeline 2<br/>Preprocessing ML | Pipeline 3<br/>PCA | Pipeline 4<br/>Histogram | Pipeline 5<br/>Augmentation | Pipeline 6<br/>CNN | Pipeline 7<br/>Transfer | Pipeline 8<br/>Split |\n",
    "|----------------|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| **ImagePathLoader** | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **TupleToDataFrame** | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImageAnalyser** | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ |\n",
    "| **DatasetStatistics** | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImageResizer** | âŒ | âœ…<br/>64x64 | âŒ | âŒ | âŒ | âœ…<br/>128x128 | âœ…<br/>224x224 | âŒ |\n",
    "| **ImageNormalizer** | âŒ | âœ… | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ |\n",
    "| **RGB_to_L** | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImageFlattener** | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImagePCA** | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **PCAVisualizer** | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImageHistogram** | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ |\n",
    "| **HistogramVisualizer** | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ |\n",
    "| **ImageAugmenter** | âŒ | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |\n",
    "| **ImageComparisonViz** | âŒ | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |\n",
    "| **TrainTestSplitter** | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Nested Pipelines (Combinaisons)\n",
    "\n",
    "```\n",
    "ğŸ“¦ NESTED 1: ML Complete (PCA)\n",
    "Pipeline 1 (Exploration) â†’ Pipeline 2 (Preprocessing ML) â†’ Pipeline 3 (Features PCA)\n",
    "â””â”€ Output: Features PCA (n_samples, 50) prÃªtes pour RandomForest/XGBoost\n",
    "\n",
    "ğŸ“¦ NESTED 2: ML Complete (Histogrammes)\n",
    "Pipeline 1 (Exploration) â†’ Pipeline 2 (Preprocessing ML) â†’ Pipeline 4 (Features Histogram)\n",
    "â””â”€ Output: Features Histogrammes (n_samples, 32) prÃªtes pour ML\n",
    "\n",
    "ğŸ“¦ NESTED 3: CNN Complete\n",
    "Pipeline 1 (Exploration) â†’ Pipeline 6 (Preprocessing CNN)\n",
    "â””â”€ Output: Images 128x128x3 RGB prÃªtes pour CNN custom\n",
    "\n",
    "ğŸ“¦ NESTED 4: Transfer Learning Complete\n",
    "Pipeline 1 (Exploration) â†’ Pipeline 7 (Preprocessing Transfer)\n",
    "â””â”€ Output: Images 224x224x3 RGB prÃªtes pour VGG16/ResNet/EfficientNet\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Workflows Typiques\n",
    "\n",
    "### ğŸ¯ Workflow ML Classique (RandomForest/XGBoost)\n",
    "```\n",
    "1. Pipeline 1 (Exploration) â†’ Analyser dataset\n",
    "2. Pipeline 8 (Split) â†’ SÃ©parer train/test\n",
    "3. Pipeline 2 (Preprocessing ML) â†’ PrÃ©parer images 64x64 gray\n",
    "4. Pipeline 3 ou 4 â†’ Extraire features PCA ou Histogrammes\n",
    "5. EntraÃ®ner RandomForest/XGBoost\n",
    "```\n",
    "\n",
    "### ğŸ§  Workflow CNN Custom\n",
    "```\n",
    "1. Pipeline 1 (Exploration) â†’ Analyser dataset\n",
    "2. Pipeline 8 (Split) â†’ SÃ©parer train/test\n",
    "3. Pipeline 5 (Augmentation) â†’ Augmenter train set (optionnel)\n",
    "4. Pipeline 6 (Preprocessing CNN) â†’ PrÃ©parer images 128x128 RGB\n",
    "5. EntraÃ®ner CNN Ã  5 blocs\n",
    "```\n",
    "\n",
    "### ğŸ”„ Workflow Transfer Learning\n",
    "```\n",
    "1. Pipeline 1 (Exploration) â†’ Analyser dataset\n",
    "2. Pipeline 8 (Split) â†’ SÃ©parer train/test\n",
    "3. Pipeline 5 (Augmentation) â†’ Augmenter train set (optionnel)\n",
    "4. Pipeline 7 (Preprocessing Transfer) â†’ PrÃ©parer images 224x224 RGB\n",
    "5. Appliquer preprocess_input() spÃ©cifique au modÃ¨le\n",
    "6. Fine-tuner VGG16/ResNet/Inception/EfficientNet\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6b3d6",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d8081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Nouveaux transformateurs disponibles!\n",
      "âœ… Imports rÃ©ussis!\n",
      "ğŸ“ RÃ©pertoire de travail: /home/lena/DS_Covid/DS_COVID/notebooks\n",
      "ğŸ†• Nouveaux transformateurs: âœ…\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Path setup\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Import transformateurs existants V3\n",
    "from src.features.St_Pipeline.Transformateurs import (\n",
    "    # Loaders\n",
    "    ImagePathLoader,\n",
    "    TupleToDataFrame,\n",
    "    \n",
    "    # Preprocessing\n",
    "    ImageResizer,\n",
    "    ImageAugmenter,\n",
    "    ImageNormalizer,\n",
    "    ImageMasker,\n",
    "    ImageFlattener,\n",
    "    ImageStandardScaler,\n",
    "    RGB_to_L,\n",
    "    \n",
    "    # Analyse et features\n",
    "    ImageAnalyser,\n",
    "    ImagePCA,\n",
    "    ImageHistogram,\n",
    "    \n",
    "    # Utilities\n",
    "    TrainTestSplitter,\n",
    ")\n",
    "\n",
    "# Import nouveaux transformateurs (Ã  crÃ©er)\n",
    "# Ces imports vont Ã©chouer jusqu'Ã  ce qu'on crÃ©e les transformateurs\n",
    "try:\n",
    "    from src.features.St_Pipeline.Transformateurs import (\n",
    "        DatasetStatistics,\n",
    "        ImageComparisonVisualizer,\n",
    "        PCAVisualizer,\n",
    "        HistogramVisualizer,\n",
    "    )\n",
    "    NEW_TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"âœ… Nouveaux transformateurs disponibles!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Nouveaux transformateurs pas encore implÃ©mentÃ©s: {e}\")\n",
    "    NEW_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "print(\"âœ… Imports rÃ©ussis!\")\n",
    "print(f\"ğŸ“ RÃ©pertoire de travail: {os.getcwd()}\")\n",
    "print(f\"ğŸ†• Nouveaux transformateurs: {'âœ…' if NEW_TRANSFORMERS_AVAILABLE else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea2e9c",
   "metadata": {},
   "source": [
    "## 2. Configuration Globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e22256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset trouvÃ©: /home/lena/DS_Covid/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\n",
      "ğŸ“Š Labels disponibles: ['Lung_Opacity', 'Normal', 'COVID', 'Viral Pneumonia']\n",
      "ğŸ“Š Nombre de classes: 4\n",
      "   â€¢ Lung_Opacity: 6012 images\n",
      "   â€¢ Normal: 10192 images\n",
      "   â€¢ COVID: 3616 images\n",
      "   â€¢ Viral Pneumonia: 1345 images\n",
      "ğŸ“Š Total: 21165 images\n",
      "\n",
      "================================================================================\n",
      "âœ… Configuration chargÃ©e avec succÃ¨s!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION GLOBALE\n",
    "# =============================================================================\n",
    "\n",
    "# Chemins\n",
    "ROOT_DIR = \"/home/lena/DS_Covid/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset\"\n",
    "MODELS_DIR = \"../models\"\n",
    "OUTPUTS_DIR = \"../outputs\"\n",
    "\n",
    "# CrÃ©er les dossiers\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "# ParamÃ¨tres gÃ©nÃ©raux\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.15  # Pour deep learning\n",
    "\n",
    "# ParamÃ¨tres images\n",
    "IMG_SIZE_ML = (64, 64)          # Pour ML classique\n",
    "IMG_SIZE_CNN = (128, 128)        # Pour CNN custom\n",
    "IMG_SIZE_TRANSFER = (224, 224)   # Pour transfer learning\n",
    "\n",
    "# ParamÃ¨tres features\n",
    "PCA_COMPONENTS = 50\n",
    "HISTOGRAM_BINS = 32\n",
    "\n",
    "# Classes du dataset\n",
    "CLASSES = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "\n",
    "# VÃ©rification dataset\n",
    "if os.path.exists(ROOT_DIR):\n",
    "    print(f\"âœ… Dataset trouvÃ©: {ROOT_DIR}\")\n",
    "    labels = [d for d in os.listdir(ROOT_DIR) \n",
    "              if os.path.isdir(os.path.join(ROOT_DIR, d, 'images'))]\n",
    "    print(f\"ğŸ“Š Labels disponibles: {labels}\")\n",
    "    print(f\"ğŸ“Š Nombre de classes: {len(labels)}\")\n",
    "    \n",
    "    # Compter images\n",
    "    total_images = 0\n",
    "    for label in labels:\n",
    "        img_dir = os.path.join(ROOT_DIR, label, 'images')\n",
    "        if os.path.exists(img_dir):\n",
    "            n_images = len([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "            total_images += n_images\n",
    "            print(f\"   â€¢ {label}: {n_images} images\")\n",
    "    print(f\"ğŸ“Š Total: {total_images} images\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset introuvable: {ROOT_DIR}\")\n",
    "    raise FileNotFoundError(\"Dataset COVID-19 manquant\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Configuration chargÃ©e avec succÃ¨s!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eddb62",
   "metadata": {},
   "source": [
    "## 3. ğŸ“Š PIPELINE 1: Exploration Dataset\n",
    "\n",
    "**Objectif**: Analyser le dataset et gÃ©nÃ©rer statistiques/visualisations\n",
    "\n",
    "**Composants**:\n",
    "1. ImagePathLoader - Scan directories\n",
    "2. TupleToDataFrame - Convert to DataFrame\n",
    "3. ImageAnalyser - Analyze metadata (NO image loading)\n",
    "4. DatasetStatistics - Compute stats + visualizations (NOUVEAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2137747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” PIPELINE 1: EXPLORATION DATASET\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_1_exploration.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_* ou Pipeline_CNN_*\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. loader: ImagePathLoader\n",
      "  2. tuple_to_df: TupleToDataFrame\n",
      "  3. analyzer: ImageAnalyser\n",
      "  4. statistics: DatasetStatistics\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  df_metadata = pipeline_exploration.fit_transform(None)\n",
      "  # GÃ©nÃ¨re: DataFrame + visualisations statistiques\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” PIPELINE 1: EXPLORATION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if NEW_TRANSFORMERS_AVAILABLE:\n",
    "    pipeline_exploration = Pipeline([\n",
    "        ('loader', ImagePathLoader(root_dir=ROOT_DIR, verbose=True, use_streamlit=False)),\n",
    "        ('tuple_to_df', TupleToDataFrame(verbose=True, use_streamlit=False)),\n",
    "        ('analyzer', ImageAnalyser(load_images=False, analyze_masks=True, verbose=True, use_streamlit=False)),\n",
    "        ('statistics', DatasetStatistics(compute_pixel_stats=False, verbose=True, use_streamlit=False)),\n",
    "    ])\n",
    "else:\n",
    "    # Version sans DatasetStatistics (pour l'instant)\n",
    "    pipeline_exploration = Pipeline([\n",
    "        ('loader', ImagePathLoader(root_dir=ROOT_DIR, verbose=True, use_streamlit=False)),\n",
    "        ('tuple_to_df', TupleToDataFrame(verbose=True, use_streamlit=False)),\n",
    "        ('analyzer', ImageAnalyser(load_images=False, analyze_masks=True, verbose=True, use_streamlit=False)),\n",
    "    ])\n",
    "    print(\"âš ï¸ DatasetStatistics pas encore implÃ©mentÃ© - pipeline partiel\")\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_1_exploration.pkl')\n",
    "joblib.dump(pipeline_exploration, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_* ou Pipeline_CNN_*\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_exploration.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  df_metadata = pipeline_exploration.fit_transform(None)\")\n",
    "print(\"  # GÃ©nÃ¨re: DataFrame + visualisations statistiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84793311",
   "metadata": {},
   "source": [
    "## 4. ğŸ”„ PIPELINE 2: Preprocessing pour ML Classique\n",
    "\n",
    "**Objectif**: PrÃ©parer images pour RandomForest/XGBoost (64x64, grayscale)\n",
    "\n",
    "**Composants**:\n",
    "1. ImageAnalyser - Load images in memory\n",
    "2. ImageResizer - Resize to 64x64\n",
    "3. ImageNormalizer - Normalize to [0,1]\n",
    "4. RGB_to_L - Convert to grayscale\n",
    "5. ImageComparisonVisualizer - Show before/after (NOUVEAU - optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ed8327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ–¼ï¸ PIPELINE 2: PREPROCESSING ML CLASSIQUE\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_2_preprocessing_ml.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_PCA ou Pipeline_ML_Histogram\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. loader: ImageAnalyser\n",
      "  2. resizer: ImageResizer\n",
      "  3. normalizer: ImageNormalizer\n",
      "  4. gray: RGB_to_L\n",
      "  5. visualizer: ImageComparisonVisualizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  df_processed = pipeline_preprocessing_ml.fit_transform(df_train)\n",
      "  # Images: 64x64, grayscale, normalisÃ©es\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ–¼ï¸ PIPELINE 2: PREPROCESSING ML CLASSIQUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "transformers_list = [\n",
    "    ('loader', ImageAnalyser(load_images=True, analyze_masks=False, verbose=True, use_streamlit=False)),\n",
    "    ('resizer', ImageResizer(img_size=IMG_SIZE_ML, verbose=True, use_streamlit=False)),\n",
    "    ('normalizer', ImageNormalizer(verbose=True, use_streamlit=False)),\n",
    "    ('gray', RGB_to_L(verbose=True, use_streamlit=False)),\n",
    "]\n",
    "\n",
    "# Ajouter visualisateur si disponible\n",
    "if NEW_TRANSFORMERS_AVAILABLE:\n",
    "    transformers_list.append(\n",
    "        ('visualizer', ImageComparisonVisualizer(n_samples=6, comparison_mode='side-by-side', \n",
    "                                                  verbose=True, use_streamlit=False))\n",
    "    )\n",
    "\n",
    "pipeline_preprocessing_ml = Pipeline(transformers_list)\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_2_preprocessing_ml.pkl')\n",
    "joblib.dump(pipeline_preprocessing_ml, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_PCA ou Pipeline_ML_Histogram\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_preprocessing_ml.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  df_processed = pipeline_preprocessing_ml.fit_transform(df_train)\")\n",
    "print(\"  # Images: 64x64, grayscale, normalisÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8da9eb",
   "metadata": {},
   "source": [
    "## 5. ğŸ”¬ PIPELINE 3: Features PCA avec Visualisations\n",
    "\n",
    "**Objectif**: Extraire features PCA + visualiser\n",
    "\n",
    "**Composants**:\n",
    "1. ImageFlattener - Flatten images to 1D\n",
    "2. ImagePCA - Apply PCA (50 components)\n",
    "3. PCAVisualizer - Visualize variance + projections 2D/3D (NOUVEAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c8964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ PIPELINE 3: FEATURES PCA\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_3_features_pca.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_PCA\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. flattener: ImageFlattener\n",
      "  2. pca: ImagePCA\n",
      "  3. pca_viz: PCAVisualizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  X_pca = pipeline_features_pca.fit_transform(df_processed, y=labels)\n",
      "  # Shape: (n_samples, 50)\n",
      "  # GÃ©nÃ¨re: variance plot, 2D/3D projections\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ PIPELINE 3: FEATURES PCA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "transformers_list = [\n",
    "    ('flattener', ImageFlattener(verbose=True, use_streamlit=False)),\n",
    "    ('pca', ImagePCA(n_components=PCA_COMPONENTS, verbose=True, use_streamlit=False)),\n",
    "]\n",
    "\n",
    "# Ajouter visualisateur si disponible\n",
    "if NEW_TRANSFORMERS_AVAILABLE:\n",
    "    transformers_list.append(\n",
    "        ('pca_viz', PCAVisualizer(n_components=PCA_COMPONENTS, projection_mode='both', \n",
    "                                  verbose=True, use_streamlit=False))\n",
    "    )\n",
    "\n",
    "pipeline_features_pca = Pipeline(transformers_list)\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_3_features_pca.pkl')\n",
    "joblib.dump(pipeline_features_pca, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_PCA\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_features_pca.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  X_pca = pipeline_features_pca.fit_transform(df_processed, y=labels)\")\n",
    "print(\"  # Shape: (n_samples, 50)\")\n",
    "print(\"  # GÃ©nÃ¨re: variance plot, 2D/3D projections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f06129",
   "metadata": {},
   "source": [
    "## 6. ğŸ“Š PIPELINE 4: Features Histogrammes avec Visualisations\n",
    "\n",
    "**Objectif**: Extraire histogrammes + visualiser\n",
    "\n",
    "**Composants**:\n",
    "1. ImageHistogram - Compute intensity histograms\n",
    "2. HistogramVisualizer - Visualize distributions per class (NOUVEAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba27e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š PIPELINE 4: FEATURES HISTOGRAMMES\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_4_features_histogram.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_Histogram\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. histogram: ImageHistogram\n",
      "  2. hist_viz: HistogramVisualizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  X_hist = pipeline_features_histogram.fit_transform(df_processed, y=labels)\n",
      "  # Shape: (n_samples, 32)\n",
      "  # GÃ©nÃ¨re: histogrammes moyens par classe, heatmaps\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š PIPELINE 4: FEATURES HISTOGRAMMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "transformers_list = [\n",
    "    ('histogram', ImageHistogram(bins=HISTOGRAM_BINS, verbose=True, use_streamlit=False)),\n",
    "]\n",
    "\n",
    "# Ajouter visualisateur si disponible\n",
    "if NEW_TRANSFORMERS_AVAILABLE:\n",
    "    transformers_list.append(\n",
    "        ('hist_viz', HistogramVisualizer(n_bins=HISTOGRAM_BINS, plot_mode='all',\n",
    "                                         verbose=True, use_streamlit=False))\n",
    "    )\n",
    "\n",
    "pipeline_features_histogram = Pipeline(transformers_list)\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_4_features_histogram.pkl')\n",
    "joblib.dump(pipeline_features_histogram, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_ML_Histogram\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_features_histogram.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  X_hist = pipeline_features_histogram.fit_transform(df_processed, y=labels)\")\n",
    "print(\"  # Shape: (n_samples, 32)\")\n",
    "print(\"  # GÃ©nÃ¨re: histogrammes moyens par classe, heatmaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67ecfd",
   "metadata": {},
   "source": [
    "## 7. ğŸ¨ PIPELINE 5: Data Augmentation\n",
    "\n",
    "**Objectif**: Augmenter dataset avec transformations\n",
    "\n",
    "**Composants**:\n",
    "1. ImageAugmenter - Random transformations\n",
    "2. ImageMasker - Apply lung masks (optionnel)\n",
    "3. ImageComparisonVisualizer - Show augmentation effects (NOUVEAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f990a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¨ PIPELINE 5: DATA AUGMENTATION\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_5_augmentation.pkl\n",
      "ğŸ’¡ Section optionnelle - appliquer sur train set avant preprocessing\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. augmenter: ImageAugmenter\n",
      "  2. aug_viz: ImageComparisonVisualizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  df_augmented = pipeline_augmentation.fit_transform(df_train)\n",
      "  # Applique: rotation, brightness, noise, zoom\n",
      "  # GÃ©nÃ¨re: visualisations avant/aprÃ¨s/diffÃ©rence\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¨ PIPELINE 5: DATA AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "transformers_list = [\n",
    "    ('augmenter', ImageAugmenter(\n",
    "        flip_horizontal=False,  # Medical images: no horizontal flip\n",
    "        flip_vertical=False,\n",
    "        rotation_range=15,\n",
    "        brightness_range=(0.8, 1.2),\n",
    "        noise_std=0.02,\n",
    "        zoom_range=(0.95, 1.05),\n",
    "        probability=0.5,\n",
    "        seed=RANDOM_STATE,\n",
    "        verbose=True,\n",
    "        use_streamlit=False\n",
    "    )),\n",
    "    # ImageMasker optionnel (dÃ©commenter si besoin)\n",
    "    # ('masker', ImageMasker(verbose=True, use_streamlit=False)),\n",
    "]\n",
    "\n",
    "# Ajouter visualisateur si disponible\n",
    "if NEW_TRANSFORMERS_AVAILABLE:\n",
    "    transformers_list.append(\n",
    "        ('aug_viz', ImageComparisonVisualizer(n_samples=6, comparison_mode='difference',\n",
    "                                               verbose=True, use_streamlit=False))\n",
    "    )\n",
    "\n",
    "pipeline_augmentation = Pipeline(transformers_list)\n",
    "\n",
    "# Sauvegarder (section, optionnelle)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_5_augmentation.pkl')\n",
    "joblib.dump(pipeline_augmentation, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"ğŸ’¡ Section optionnelle - appliquer sur train set avant preprocessing\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_augmentation.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  df_augmented = pipeline_augmentation.fit_transform(df_train)\")\n",
    "print(\"  # Applique: rotation, brightness, noise, zoom\")\n",
    "print(\"  # GÃ©nÃ¨re: visualisations avant/aprÃ¨s/diffÃ©rence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811cb16",
   "metadata": {},
   "source": [
    "## 8. ğŸ§  PIPELINE 6: Preprocessing pour CNN Custom\n",
    "\n",
    "**Objectif**: PrÃ©parer images pour deep learning (128x128, RGB)\n",
    "\n",
    "**Composants**:\n",
    "1. ImageAnalyser - Load images\n",
    "2. ImageResizer - Resize to 128x128\n",
    "3. ImageNormalizer - Normalize to [0,1]\n",
    "\n",
    "Note: Garde RGB (3 channels) pour CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f642c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ§  PIPELINE 6: PREPROCESSING CNN CUSTOM\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_6_preprocessing_cnn.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_CNN\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. loader: ImageAnalyser\n",
      "  2. resizer: ImageResizer\n",
      "  3. normalizer: ImageNormalizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  df_cnn = pipeline_preprocessing_cnn.fit_transform(df_train)\n",
      "  # Images: 128x128x3, RGB, normalisÃ©es\n",
      "  # PrÃªt pour CNN custom Ã  5 blocs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§  PIPELINE 6: PREPROCESSING CNN CUSTOM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipeline_preprocessing_cnn = Pipeline([\n",
    "    ('loader', ImageAnalyser(load_images=True, analyze_masks=False, verbose=True, use_streamlit=False)),\n",
    "    ('resizer', ImageResizer(img_size=IMG_SIZE_CNN, verbose=True, use_streamlit=False)),\n",
    "    ('normalizer', ImageNormalizer(verbose=True, use_streamlit=False)),\n",
    "    # Pas de RGB_to_L - on garde les 3 channels\n",
    "])\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_6_preprocessing_cnn.pkl')\n",
    "joblib.dump(pipeline_preprocessing_cnn, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_CNN\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_preprocessing_cnn.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  df_cnn = pipeline_preprocessing_cnn.fit_transform(df_train)\")\n",
    "print(\"  # Images: 128x128x3, RGB, normalisÃ©es\")\n",
    "print(\"  # PrÃªt pour CNN custom Ã  5 blocs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84eb350",
   "metadata": {},
   "source": [
    "## 9. ğŸ”„ PIPELINE 7: Preprocessing pour Transfer Learning\n",
    "\n",
    "**Objectif**: PrÃ©parer images pour VGG16/ResNet/Inception/EfficientNet (224x224)\n",
    "\n",
    "**Composants**:\n",
    "1. ImageAnalyser - Load images\n",
    "2. ImageResizer - Resize to 224x224\n",
    "3. ImageNormalizer - Normalize to [0,1]\n",
    "\n",
    "Note: Preprocessing spÃ©cifique au modÃ¨le (preprocess_input) sera fait plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4b3b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ PIPELINE 7: PREPROCESSING TRANSFER LEARNING\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_7_preprocessing_transfer.pkl\n",
      "âš ï¸  Section non utilisable seule - utiliser Pipeline_Transfer\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. loader: ImageAnalyser\n",
      "  2. resizer: ImageResizer\n",
      "  3. normalizer: ImageNormalizer\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  df_transfer = pipeline_preprocessing_transfer.fit_transform(df_train)\n",
      "  # Images: 224x224x3, RGB, normalisÃ©es\n",
      "  # Puis appliquer: keras.applications.<model>.preprocess_input()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ PIPELINE 7: PREPROCESSING TRANSFER LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipeline_preprocessing_transfer = Pipeline([\n",
    "    ('loader', ImageAnalyser(load_images=True, analyze_masks=False, verbose=True, use_streamlit=False)),\n",
    "    ('resizer', ImageResizer(img_size=IMG_SIZE_TRANSFER, verbose=True, use_streamlit=False)),\n",
    "    ('normalizer', ImageNormalizer(verbose=True, use_streamlit=False)),\n",
    "    # Pas de RGB_to_L - modÃ¨les prÃ©-entraÃ®nÃ©s attendent RGB\n",
    "])\n",
    "\n",
    "# Sauvegarder (section, pas utilisable seule)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_7_preprocessing_transfer.pkl')\n",
    "joblib.dump(pipeline_preprocessing_transfer, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âš ï¸  Section non utilisable seule - utiliser Pipeline_Transfer\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_preprocessing_transfer.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  df_transfer = pipeline_preprocessing_transfer.fit_transform(df_train)\")\n",
    "print(\"  # Images: 224x224x3, RGB, normalisÃ©es\")\n",
    "print(\"  # Puis appliquer: keras.applications.<model>.preprocess_input()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968db87",
   "metadata": {},
   "source": [
    "## 10. ğŸ¯ PIPELINE 8: Split Train/Validation/Test\n",
    "\n",
    "**Objectif**: SÃ©parer dataset de maniÃ¨re stratifiÃ©e\n",
    "\n",
    "**Composant**:\n",
    "1. TrainTestSplitter - Stratified split\n",
    "\n",
    "Note: Pour deep learning, on fera un second split train â†’ train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db5fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ PIPELINE 8: TRAIN/TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "âœ… Section sauvegardÃ©e: ../models/Section_8_split.pkl\n",
      "âœ… Section utilisable directement sur DataFrame\n",
      "\n",
      "ğŸ“‹ Structure:\n",
      "  1. splitter: TrainTestSplitter\n",
      "\n",
      "ğŸ’¡ Usage:\n",
      "  splits = pipeline_split.fit_transform(df_metadata)\n",
      "  df_train, y_train = splits['train']\n",
      "  df_test, y_test = splits['test']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ PIPELINE 8: TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipeline_split = Pipeline([\n",
    "    ('splitter', TrainTestSplitter(test_size=TEST_SIZE, random_state=RANDOM_STATE, \n",
    "                                   verbose=True, use_streamlit=False)),\n",
    "])\n",
    "\n",
    "# Sauvegarder (section utilisable)\n",
    "pipeline_path = os.path.join(MODELS_DIR, 'Section_8_split.pkl')\n",
    "joblib.dump(pipeline_split, pipeline_path)\n",
    "print(f\"\\nâœ… Section sauvegardÃ©e: {pipeline_path}\")\n",
    "print(\"âœ… Section utilisable directement sur DataFrame\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Structure:\")\n",
    "for idx, (name, transformer) in enumerate(pipeline_split.steps, 1):\n",
    "    print(f\"  {idx}. {name}: {transformer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Usage:\")\n",
    "print(\"  splits = pipeline_split.fit_transform(df_metadata)\")\n",
    "print(\"  df_train, y_train = splits['train']\")\n",
    "print(\"  df_test, y_test = splits['test']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e675cfe",
   "metadata": {},
   "source": [
    "## 11. ğŸ“¦ NESTED PIPELINES - Combinaisons ComplÃ¨tes\n",
    "\n",
    "**Objectif**: CrÃ©er pipelines end-to-end pour Streamlit\n",
    "\n",
    "Ces pipelines combinent plusieurs Ã©tapes pour crÃ©er des workflows complets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541c072",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2591955270.py, line 73)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"=\"*80)print(\"=\"*80)\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“¦ CRÃ‰ATION NESTED PIPELINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -------------------------\n",
    "# NESTED 1: ML Complete (Exploration â†’ Preprocessing â†’ PCA)\n",
    "# -------------------------\n",
    "print(\"\\nğŸ”— Nested Pipeline 1: ML Complete (PCA)\")\n",
    "\n",
    "nested_ml_pca = Pipeline([\n",
    "    ('exploration', pipeline_exploration),\n",
    "    ('preprocessing', pipeline_preprocessing_ml),\n",
    "    ('features', pipeline_features_pca),\n",
    "])\n",
    "\n",
    "path = os.path.join(MODELS_DIR, 'Pipeline_ML_PCA_Complete.pkl')\n",
    "joblib.dump(nested_ml_pca, path)\n",
    "print(f\"âœ… Pipeline complet sauvegardÃ©: {path}\")\n",
    "print(\"   ğŸ“¦ UTILISABLE DIRECTEMENT\")\n",
    "print(f\"   Ã‰tapes totales: {len(pipeline_exploration.steps)} + {len(pipeline_preprocessing_ml.steps)} + {len(pipeline_features_pca.steps)}\")\n",
    "\n",
    "# -------------------------\n",
    "# NESTED 2: ML Complete (Exploration â†’ Preprocessing â†’ Histogrammes)\n",
    "# -------------------------\n",
    "print(\"\\nğŸ”— Nested Pipeline 2: ML Complete (Histogrammes)\")\n",
    "\n",
    "nested_ml_hist = Pipeline([\n",
    "    ('exploration', pipeline_exploration),\n",
    "    ('preprocessing', pipeline_preprocessing_ml),\n",
    "    ('features', pipeline_features_histogram),\n",
    "])\n",
    "path = os.path.join(MODELS_DIR, 'Pipeline_ML_Histogram_Complete.pkl')\n",
    "path = os.path.join(MODELS_DIR, 'nested_pipeline_ml_hist_complete.pkl')\n",
    "print(f\"âœ… Pipeline complet sauvegardÃ©: {path}\")\n",
    "print(\"   ğŸ“¦ UTILISABLE DIRECTEMENT\")\n",
    "print(f\"   Ã‰tapes totales: {len(pipeline_exploration.steps)} + {len(pipeline_preprocessing_ml.steps)} + {len(pipeline_features_histogram.steps)}\")\n",
    "\n",
    "# -------------------------\n",
    "# NESTED 3: CNN Complete (Exploration â†’ Preprocessing CNN)\n",
    "# -------------------------\n",
    "print(\"\\nğŸ”— Nested Pipeline 3: CNN Complete\")\n",
    "\n",
    "nested_cnn = Pipeline([\n",
    "    ('exploration', pipeline_exploration),\n",
    "    ('preprocessing', pipeline_preprocessing_cnn),\n",
    "])\n",
    "path = os.path.join(MODELS_DIR, 'Pipeline_CNN_Complete.pkl')\n",
    "path = os.path.join(MODELS_DIR, 'nested_pipeline_cnn_complete.pkl')\n",
    "print(f\"âœ… Pipeline complet sauvegardÃ©: {path}\")\n",
    "print(\"   ğŸ“¦ UTILISABLE DIRECTEMENT\")\n",
    "print(f\"   Ã‰tapes totales: {len(pipeline_exploration.steps)} + {len(pipeline_preprocessing_cnn.steps)}\")\n",
    "\n",
    "# -------------------------\n",
    "# NESTED 4: Transfer Learning Complete\n",
    "# -------------------------\n",
    "print(\"\\nğŸ”— Nested Pipeline 4: Transfer Learning Complete\")\n",
    "\n",
    "nested_transfer = Pipeline([\n",
    "    ('exploration', pipeline_exploration),\n",
    "    ('preprocessing', pipeline_preprocessing_transfer),\n",
    "])\n",
    "path = os.path.join(MODELS_DIR, 'Pipeline_Transfer_Complete.pkl')\n",
    "path = os.path.join(MODELS_DIR, 'nested_pipeline_transfer_complete.pkl')\n",
    "print(f\"âœ… Pipeline complet sauvegardÃ©: {path}\")\n",
    "print(\"   ğŸ“¦ UTILISABLE DIRECTEMENT\")\n",
    "print(f\"   Ã‰tapes totales: {len(pipeline_exploration.steps)} + {len(pipeline_preprocessing_transfer.steps)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TOUS LES NESTED PIPELINES CRÃ‰Ã‰S\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"=\"*80)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00465f4",
   "metadata": {},
   "source": [
    "## 12. ğŸ“Š RÃ©sumÃ© des Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RÃ‰SUMÃ‰ DES PIPELINES V4\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… SECTIONS SAUVEGARDÃ‰ES (composants)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sections_summary = {\n",
    "    \"Section 1 - Exploration\": {\n",
    "        \"file\": \"Section_1_exploration.pkl\",\n",
    "        \"usage\": \"Composant: Analyse dataset\",\n",
    "        \"steps\": len(pipeline_exploration.steps)\n",
    "    },\n",
    "    \"Section 2 - Preprocessing ML\": {\n",
    "        \"file\": \"Section_2_preprocessing_ml.pkl\",\n",
    "        \"usage\": \"Composant: Preprocessing ML (64x64, gray)\",\n",
    "        \"steps\": len(pipeline_preprocessing_ml.steps)\n",
    "    },\n",
    "    \"Section 3 - Features PCA\": {\n",
    "        \"file\": \"Section_3_features_pca.pkl\",\n",
    "        \"usage\": \"Composant: Extraction PCA\",\n",
    "        \"steps\": len(pipeline_features_pca.steps)\n",
    "    },\n",
    "    \"Section 4 - Features Histogrammes\": {\n",
    "        \"file\": \"Section_4_features_histogram.pkl\",\n",
    "        \"usage\": \"Composant: Extraction histogrammes\",\n",
    "        \"steps\": len(pipeline_features_histogram.steps)\n",
    "    },\n",
    "    \"Section 5 - Augmentation\": {\n",
    "        \"file\": \"Section_5_augmentation.pkl\",\n",
    "        \"usage\": \"Composant optionnel: Augmentation\",\n",
    "        \"steps\": len(pipeline_augmentation.steps)\n",
    "    },\n",
    "    \"Section 6 - Preprocessing CNN\": {\n",
    "        \"file\": \"Section_6_preprocessing_cnn.pkl\",\n",
    "        \"usage\": \"Composant: Preprocessing CNN (128x128, RGB)\",\n",
    "        \"steps\": len(pipeline_preprocessing_cnn.steps)\n",
    "    },\n",
    "    \"Section 7 - Preprocessing Transfer\": {\n",
    "        \"file\": \"Section_7_preprocessing_transfer.pkl\",\n",
    "        \"usage\": \"Composant: Preprocessing Transfer (224x224, RGB)\",\n",
    "        \"steps\": len(pipeline_preprocessing_transfer.steps)\n",
    "    },\n",
    "    \"Section 8 - Split\": {\n",
    "        \"file\": \"Section_8_split.pkl\",\n",
    "        \"usage\": \"âœ… Utilisable: Split train/test\",\n",
    "        \"steps\": len(pipeline_split.steps)\n",
    "    },\n",
    "}\n",
    "\n",
    "for name, info in sections_summary.items():\n",
    "    marker = \"âœ…\" if \"Utilisable\" in info['usage'] else \"âš™ï¸\"\n",
    "    print(f\"\\n{marker} {name}\")\n",
    "    print(f\"   Fichier: {info['file']}\")\n",
    "    print(f\"   Ã‰tapes: {info['steps']}\")\n",
    "    print(f\"   Usage: {info['usage']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ PIPELINES COMPLETS (PRÃŠTS Ã€ L'EMPLOI)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pipelines_complete = {\n",
    "    \"Pipeline_ML_PCA_Complete\": {\n",
    "        \"file\": \"Pipeline_ML_PCA_Complete.pkl\",\n",
    "        \"usage\": \"ML avec features PCA (RandomForest/XGBoost)\",\n",
    "        \"output\": \"Features PCA (n_samples, 50)\",\n",
    "        \"workflow\": \"Exploration â†’ Preprocessing ML â†’ PCA\"\n",
    "    },\n",
    "    \"Pipeline_ML_Histogram_Complete\": {\n",
    "        \"file\": \"Pipeline_ML_Histogram_Complete.pkl\",\n",
    "        \"usage\": \"ML avec features Histogrammes\",\n",
    "        \"output\": \"Features Histogrammes (n_samples, 32)\",\n",
    "        \"workflow\": \"Exploration â†’ Preprocessing ML â†’ Histogram\"\n",
    "    },\n",
    "    \"Pipeline_CNN_Complete\": {\n",
    "        \"file\": \"Pipeline_CNN_Complete.pkl\",\n",
    "        \"usage\": \"Deep Learning CNN custom\",\n",
    "        \"output\": \"Images 128x128x3 RGB normalisÃ©es\",\n",
    "        \"workflow\": \"Exploration â†’ Preprocessing CNN\"\n",
    "    },\n",
    "    \"Pipeline_Transfer_Complete\": {\n",
    "        \"file\": \"Pipeline_Transfer_Complete.pkl\",\n",
    "        \"usage\": \"Transfer Learning (VGG16/ResNet/EfficientNet)\",\n",
    "        \"output\": \"Images 224x224x3 RGB normalisÃ©es\",\n",
    "        \"workflow\": \"Exploration â†’ Preprocessing Transfer\"\n",
    "    },\n",
    "}\n",
    "\n",
    "for name, info in pipelines_complete.items():\n",
    "    print(f\"\\nğŸš€ {name}\")\n",
    "    print(f\"   ğŸ“¦ Fichier: {info['file']}\")\n",
    "    print(f\"   ğŸ¯ Usage: {info['usage']}\")\n",
    "    print(f\"   ğŸ“¤ Output: {info['output']}\")\n",
    "    print(f\"   ğŸ”„ Workflow: {info['workflow']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… NOTEBOOK V4 TERMINÃ‰\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“ Tous les fichiers sauvegardÃ©s dans: {MODELS_DIR}\")\n",
    "print(f\"   â€¢ Section_*.pkl = Composants (non utilisables seuls)\")\n",
    "print(f\"   â€¢ Pipeline_*.pkl = Pipelines complets (PRÃŠTS Ã€ L'EMPLOI)\")\n",
    "print(f\"\\nğŸ“ Outputs dans: {OUTPUTS_DIR}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ UTILISATION:\")\n",
    "print(\"  # Charger un pipeline complet\")\n",
    "print(\"  import joblib\")\n",
    "print(\"  pipeline = joblib.load('../models/Pipeline_ML_PCA_Complete.pkl')\")\n",
    "print(\"  \")\n",
    "print(\"  # ExÃ©cuter end-to-end\")\n",
    "print(\"  X_features = pipeline.fit_transform(None)\")\n",
    "print(\"  \")\n",
    "print(\"  # EntraÃ®ner modÃ¨le ML\")\n",
    "print(\"  from sklearn.ensemble import RandomForestClassifier\")\n",
    "print(\"  rf = RandomForestClassifier()\")\n",
    "print(\"  rf.fit(X_features, y)\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\nâ° GÃ©nÃ©rÃ© le: {timestamp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
