{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Complete Exploratory Data Analysis - COVID-19 Radiography Dataset\n",
    "\n",
    "This notebook performs a comprehensive exploratory data analysis on the COVID-19 radiography dataset including:\n",
    "\n",
    "- Data loading and validation\n",
    "- Basic visualizations (distributions, samples)\n",
    "- Deep learning embeddings extraction (ResNet50)\n",
    "- Dimensionality reduction (PCA, UMAP, t-SNE)\n",
    "- Clustering analysis (KMeans, DBSCAN)\n",
    "- Advanced visualizations (Grad-CAM, similarity matrices)\n",
    "\n",
    "**Important**: This notebook is designed to work both locally and on Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_colab"
   },
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running on Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Colab environment detected\")\n",
    "else:\n",
    "    print(\"Local environment detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone repository or navigate to it\n",
    "    REPO_PATH = '/content/DS_COVID'\n",
    "    \n",
    "    if not os.path.exists(REPO_PATH):\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/L-Poca/DS_COVID.git {REPO_PATH}\n",
    "    \n",
    "    os.chdir(REPO_PATH)\n",
    "    sys.path.insert(0, REPO_PATH)\n",
    "    \n",
    "    # Set data paths (adjust these for your Drive structure)\n",
    "    BASE_PATH = '/content/drive/MyDrive/DS_COVID/data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset'\n",
    "    METADATA_PATH = f'{REPO_PATH}/metadata'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/DS_COVID/outputs'\n",
    "else:\n",
    "    # Local paths\n",
    "    REPO_PATH = os.path.abspath('..')\n",
    "    sys.path.insert(0, REPO_PATH)\n",
    "    \n",
    "    BASE_PATH = 'data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset'\n",
    "    METADATA_PATH = 'metadata'\n",
    "    OUTPUT_DIR = 'outputs'\n",
    "\n",
    "print(f\"Repository path: {REPO_PATH}\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    print(\"Installing Colab dependencies...\")\n",
    "    !pip install -q -r {REPO_PATH}/requirements-colab.txt\n",
    "else:\n",
    "    print(\"Installing local dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline"
   },
   "source": [
    "## 2. Run Complete EDA Pipeline\n",
    "\n",
    "This section runs the complete automated pipeline that performs all analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_paths"
   },
   "outputs": [],
   "source": [
    "# Verify paths exist\n",
    "base_path_obj = Path(BASE_PATH)\n",
    "metadata_path_obj = Path(METADATA_PATH)\n",
    "\n",
    "print(f\"Base path exists: {base_path_obj.exists()}\")\n",
    "print(f\"Metadata path exists: {metadata_path_obj.exists()}\")\n",
    "\n",
    "if base_path_obj.exists():\n",
    "    print(f\"\\nContents of base path:\")\n",
    "    for item in base_path_obj.iterdir():\n",
    "        print(f\"  - {item.name}\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Base path does not exist: {BASE_PATH}\")\n",
    "    print(\"Please adjust BASE_PATH in the cell above to point to your dataset.\")\n",
    "\n",
    "if metadata_path_obj.exists():\n",
    "    print(f\"\\nMetadata files:\")\n",
    "    for item in metadata_path_obj.glob(\"*.xlsx\"):\n",
    "        print(f\"  - {item.name}\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Metadata path does not exist: {METADATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline"
   },
   "outputs": [],
   "source": [
    "# Import and run pipeline\n",
    "from src.explorationdata.pipeline.pipeline_runner import EDAPipeline\n",
    "\n",
    "# Configure pipeline\n",
    "MAX_IMAGES_PER_CLASS = None  # Set to a number for testing (e.g., 100), None for full dataset\n",
    "DEVICE = None  # Auto-detect (will use CUDA if available)\n",
    "\n",
    "print(\"Initializing EDA pipeline...\")\n",
    "pipeline = EDAPipeline(\n",
    "    base_path=BASE_PATH,\n",
    "    metadata_path=METADATA_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    "    max_images_per_class=MAX_IMAGES_PER_CLASS\n",
    ")\n",
    "\n",
    "print(\"\\nRunning full pipeline...\")\n",
    "print(\"This may take 20+ minutes depending on dataset size and hardware.\")\n",
    "print(\"Progress will be logged below.\\n\")\n",
    "\n",
    "success = pipeline.run_full_pipeline()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ Pipeline completed successfully!\")\n",
    "    print(f\"✓ Results saved to: {pipeline.output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✗ Pipeline failed. Check logs for details.\")\n",
    "    print(f\"✗ Partial results may be in: {pipeline.output_dir}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 3. View Results\n",
    "\n",
    "Let's examine some of the generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_summary"
   },
   "outputs": [],
   "source": [
    "# Load summary\n",
    "import json\n",
    "\n",
    "summary_path = Path(pipeline.output_dir) / \"summary.json\"\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(\"Pipeline Summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total images: {summary.get('total_images', 'N/A')}\")\n",
    "    print(f\"Corrupted images: {summary.get('corrupted_images', 'N/A')}\")\n",
    "    print(f\"Classes: {', '.join(summary.get('classes', []))}\")\n",
    "    print(f\"Embedding shape: {summary.get('embedding_shape', 'N/A')}\")\n",
    "    print(f\"Total time: {summary.get('total_time_seconds', 0):.2f} seconds\")\n",
    "    print(\"\\nClustering Metrics:\")\n",
    "    if 'kmeans_metrics' in summary:\n",
    "        km = summary['kmeans_metrics']\n",
    "        print(f\"  KMeans - ARI: {km.get('ari', 0):.3f}, NMI: {km.get('nmi', 0):.3f}\")\n",
    "    if 'dbscan_metrics' in summary:\n",
    "        db = summary['dbscan_metrics']\n",
    "        print(f\"  DBSCAN - ARI: {db.get('ari', 0):.3f}, NMI: {db.get('nmi', 0):.3f}\")\n",
    "else:\n",
    "    print(\"Summary file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_figures"
   },
   "outputs": [],
   "source": [
    "# Display key figures\n",
    "from IPython.display import Image, display\n",
    "\n",
    "figures_dir = Path(pipeline.output_dir) / \"figures\"\n",
    "\n",
    "key_figures = [\n",
    "    \"class_distribution.png\",\n",
    "    \"sample_grid_random.png\",\n",
    "    \"image_mask_overlays.png\",\n",
    "    \"pca_scree.png\",\n",
    "    \"umap_scatter.png\",\n",
    "    \"tsne_scatter.png\",\n",
    "    \"inter_class_similarity.png\",\n",
    "    \"cluster_representatives.png\"\n",
    "]\n",
    "\n",
    "for fig_name in key_figures:\n",
    "    fig_path = figures_dir / fig_name\n",
    "    if fig_path.exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Figure: {fig_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(Image(filename=str(fig_path)))\n",
    "    else:\n",
    "        print(f\"\\nFigure not found: {fig_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_tables"
   },
   "outputs": [],
   "source": [
    "# Display key tables\n",
    "tables_dir = Path(pipeline.output_dir) / \"tables\"\n",
    "\n",
    "# Image statistics\n",
    "stats_path = tables_dir / \"image_stats.csv\"\n",
    "if stats_path.exists():\n",
    "    df_stats = pd.read_csv(stats_path)\n",
    "    print(\"Image Statistics (first 10 rows):\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_stats.head(10))\n",
    "    \n",
    "    print(\"\\nSummary Statistics by Class:\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_stats.groupby('class')[['mean', 'std', 'width', 'height']].describe())\n",
    "\n",
    "# Clustering results\n",
    "clusters_path = tables_dir / \"clusters.csv\"\n",
    "if clusters_path.exists():\n",
    "    df_clusters = pd.read_csv(clusters_path)\n",
    "    print(\"\\nClustering Results (first 10 rows):\")\n",
    "    print(\"=\"*60)\n",
    "    display(df_clusters.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "The complete EDA pipeline has been executed. All results are saved in the output directory:\n",
    "\n",
    "- **figures/**: All visualizations (PNG files)\n",
    "- **tables/**: Data tables (CSV files)\n",
    "- **embeddings.npy**: Deep learning embeddings\n",
    "- **summary.json**: Pipeline summary with metrics\n",
    "- **log.txt**: Detailed execution log\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Review the generated visualizations for insights\n",
    "2. Examine the clustering results and similarity matrices\n",
    "3. Use the embeddings for downstream tasks (classification, etc.)\n",
    "4. Iterate on the analysis with different parameters\n",
    "\n",
    "For questions or issues, please refer to the repository documentation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
