{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9cf23df",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L-Poca/Data_Pipeline/blob/rafael_cleaning/src/notebooks/transfer_learning_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0939ce",
      "metadata": {
        "id": "3c0939ce"
      },
      "source": [
        "# üß† Transfer Learning - COVID-19 Classification (Colab Pro)\n",
        "\n",
        "**Objectif :** Classifier les radiographies pulmonaires avec Transfer Learning\n",
        "\n",
        "**Mod√®les test√©s :**\n",
        "- VGG16 (ImageNet)\n",
        "- ResNet50 (ImageNet)\n",
        "- EfficientNetB0 (ImageNet)\n",
        "- InceptionV3 (ImageNet)\n",
        "\n",
        "**Dataset :** COVID-19 Radiography (4 classes)\n",
        "- COVID\n",
        "- Normal\n",
        "- Lung_Opacity\n",
        "- Viral Pneumonia\n",
        "\n",
        "**Strat√©gie :**\n",
        "1. Feature extraction (freeze base)\n",
        "2. Fine-tuning (unfreeze top layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6a6147",
      "metadata": {
        "id": "0f6a6147"
      },
      "source": [
        "## üì¶ Cellule 1 : Configuration Standalone\n",
        "\n",
        "**‚ö†Ô∏è REMPLACER CETTE CELLULE** par le contenu complet de `CELL_CONFIG_STANDALONE.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3074115a",
      "metadata": {
        "id": "3074115a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë  üéØ CELLULE DE CONFIGURATION STANDALONE - COPIER-COLLER DANS VOS NOTEBOOKS ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "INSTRUCTIONS:\n",
        "-------------\n",
        "1. Copiez TOUT le contenu de cette cellule\n",
        "2. Collez-le comme PREMI√àRE CELLULE de votre notebook\n",
        "3. Ex√©cutez la cellule\n",
        "4. Les variables sont pr√™tes √† l'emploi !\n",
        "\n",
        "Cette cellule est 100% autonome et fonctionne partout :\n",
        "‚úÖ Google Colab (clone + installe automatiquement)\n",
        "‚úÖ WSL / Linux Local\n",
        "‚úÖ Tout environnement Jupyter\n",
        "\n",
        "APR√àS EX√âCUTION, VOUS POUVEZ UTILISER:\n",
        "- config: Objet de configuration (config.batch_size, config.data_dir, etc.)\n",
        "- ENV: Environnement d√©tect√© ('colab', 'wsl', 'local')\n",
        "- Tous les imports des transformers\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS STANDARDS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# D√âTECTION AUTOMATIQUE DE L'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def detect_environment():\n",
        "    \"\"\"D√©tecte l'environnement (colab, wsl, local)\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except ImportError:\n",
        "        is_wsl = os.path.exists('/proc/version') and 'microsoft' in open('/proc/version').read().lower()\n",
        "        return \"wsl\" if is_wsl else \"local\"\n",
        "\n",
        "ENV = detect_environment()\n",
        "print(f\"üåç Environnement: {ENV.upper()}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# BOOTSTRAP COLAB (Clone + Install si n√©cessaire)\n",
        "# =============================================================================\n",
        "\n",
        "if ENV == \"colab\":\n",
        "    print(\"\\nüöÄ Bootstrap Colab...\")\n",
        "\n",
        "    os.chdir('/content')\n",
        "    if not os.path.exists('/content/Data_Pipeline'):\n",
        "        print(\"üì• Clonage du repository...\")\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/L-Poca/Data_Pipeline.git'], check=True)\n",
        "\n",
        "    os.chdir('/content/Data_Pipeline')\n",
        "\n",
        "    # Checkout de la branche rafael_cleaning\n",
        "    result = subprocess.run(\n",
        "        ['git', 'checkout', '-b', 'rafael_cleaning', 'origin/rafael_cleaning'],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        # Si la branche locale existe d√©j√†, juste switcher\n",
        "        subprocess.run(['git', 'checkout', 'rafael_cleaning'], capture_output=True)\n",
        "\n",
        "    print(\"üì¶ Installation des d√©pendances...\")\n",
        "    subprocess.run(['pip', 'install', '-r', 'requirements.txt', '--quiet'], check=True)\n",
        "\n",
        "    print(\"üì¶ Installation du package...\")\n",
        "    result = subprocess.run(['pip', 'install', '-e', '.', '--quiet'], capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"‚ö†Ô∏è Erreur installation: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Package install√©\")\n",
        "\n",
        "    print(\"üíæ Montage Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Extraction dataset\n",
        "    for archive in ['/content/drive/MyDrive/DS_COVID/archive_covid.zip']:\n",
        "        if os.path.exists(archive):\n",
        "            print(\"üì¶ Extraction dataset...\")\n",
        "            os.makedirs('./data/raw/', exist_ok=True)\n",
        "            subprocess.run(['unzip', '-o', '-q', archive, '-d', './data/raw/COVID-19_Radiography_Dataset/'])\n",
        "            break\n",
        "\n",
        "    print(\"‚úÖ Bootstrap termin√©\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION DES CHEMINS\n",
        "# =============================================================================\n",
        "\n",
        "# D√©terminer project_root selon l'environnement\n",
        "if ENV == \"colab\":\n",
        "    project_root = Path('/content/Data_Pipeline')\n",
        "elif ENV == \"wsl\":\n",
        "    project_root = Path('/home/cepa/DST/projet_DS/Data_Pipeline/Data_Pipeline')\n",
        "else:  # local\n",
        "    # Depuis un notebook dans src/notebooks/\n",
        "    project_root = Path.cwd().parent.parent\n",
        "\n",
        "# Ajouter src/ au sys.path pour les imports\n",
        "# src_path = str(project_root / 'src')\n",
        "# if src_path not in sys.path:\n",
        "#     sys.path.insert(0, src_path)\n",
        "#     print(f\"‚úÖ Chemin src/ ajout√©: {src_path}\")\n",
        "\n",
        "# Charger la configuration depuis JSON\n",
        "from src.utils.config import build_config\n",
        "\n",
        "config = build_config(project_root, ENV)\n",
        "\n",
        "# Exports pour compatibilit√© avec anciens notebooks\n",
        "data_dir = config.data_dir\n",
        "categories = config.classes\n",
        "img_size = config.img_size\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS DES TRANSFORMERS\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    from src.features.Pipelines.Transformateurs.image_loaders import ImageLoader\n",
        "    from src.features.Pipelines.Transformateurs.image_preprocessing import (\n",
        "        ImageResizer, ImageNormalizer, ImageFlattener\n",
        "    )\n",
        "    from src.features.Pipelines.Transformateurs.image_augmentation import (\n",
        "        ImageAugmenter, ImageRandomCropper\n",
        "    )\n",
        "    from src.features.Pipelines.Transformateurs.image_features import (\n",
        "        ImageHistogram, ImagePCA, ImageStandardScaler\n",
        "    )\n",
        "    print(\"‚úÖ Transformers import√©s\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur import transformers: {e}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS ML/DL\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION MATPLOTLIB\n",
        "# =============================================================================\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# =============================================================================\n",
        "# AFFICHAGE DU R√âSUM√â\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CONFIGURATION PR√äTE - Data Pipeline\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìÇ Projet: {project_root}\")\n",
        "print(f\"üìä Dataset: {data_dir}\")\n",
        "print(f\"üè∑Ô∏è Classes: {', '.join(categories)}\")\n",
        "print(f\"üéõÔ∏è Images: {img_size}\")\n",
        "print(f\"üîß Batch: {config.batch_size} | √âpoques: {config.epochs}\")\n",
        "print(f\"üìê Dataset accessible: {'‚úÖ' if data_dir.exists() else '‚ùå'}\")\n",
        "if not data_dir.exists():\n",
        "    print(f\"   ‚ö†Ô∏è Cr√©ez le dossier ou placez les donn√©es dans: {data_dir}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° Variables disponibles:\")\n",
        "print(\"   ‚Ä¢ config: Configuration compl√®te (Config object)\")\n",
        "print(\"   ‚Ä¢ project_root: Racine du projet (Path)\")\n",
        "print(\"   ‚Ä¢ data_dir: Dossier des donn√©es (Path)\")\n",
        "print(\"   ‚Ä¢ categories: Liste des 4 classes\")\n",
        "print(\"   ‚Ä¢ img_size: Taille des images (tuple)\")\n",
        "print(\"   ‚Ä¢ ENV: Environnement actuel\")\n",
        "print(\"\\nüéØ Transformers disponibles:\")\n",
        "print(\"   ‚Ä¢ ImageLoader, ImageResizer, ImageNormalizer, ImageFlattener\")\n",
        "print(\"   ‚Ä¢ ImageAugmenter, ImageRandomCropper\")\n",
        "print(\"   ‚Ä¢ ImageHistogram, ImagePCA, ImageStandardScaler\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3645cc69",
      "metadata": {
        "id": "3645cc69"
      },
      "source": [
        "## 1. V√©rification GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7083428",
      "metadata": {
        "id": "c7083428"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"V√âRIFICATION GPU\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# V√©rifier GPU\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"\\n‚úÖ GPU disponible: {len(gpus)} GPU(s)\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"   ‚Ä¢ {gpu.name}\")\n",
        "\n",
        "    # Configurer la m√©moire GPU\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(\"   ‚Ä¢ Memory growth activ√©\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Pas de GPU disponible - utilisation CPU\")\n",
        "    print(\"   Pour activer GPU sur Colab: Runtime > Change runtime type > GPU\")\n",
        "\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9eb9c1",
      "metadata": {
        "id": "4d9eb9c1"
      },
      "source": [
        "## 2. Chargement des Donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873bbb14",
      "metadata": {
        "id": "873bbb14"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CHARGEMENT DES DONN√âES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Nombre d'images par classe (Colab Pro peut g√©rer plus)\n",
        "N_IMAGES_PER_CLASS = 1000  # Ajuster selon GPU disponible\n",
        "\n",
        "image_paths = []\n",
        "labels = []\n",
        "labels_int = []\n",
        "\n",
        "for idx, cat in enumerate(categories):\n",
        "    cat_path = data_dir / cat / 'images'\n",
        "    if cat_path.exists():\n",
        "        imgs = sorted(list(cat_path.glob('*.png')))[:N_IMAGES_PER_CLASS]\n",
        "        image_paths.extend(imgs)\n",
        "        labels.extend([cat] * len(imgs))\n",
        "        labels_int.extend([idx] * len(imgs))\n",
        "        print(f\"  {cat:20s}: {len(imgs):4d} images\")\n",
        "\n",
        "labels_int = np.array(labels_int)\n",
        "\n",
        "print(f\"\\n  Total: {len(image_paths)} images\")\n",
        "print(f\"  Classes: {len(categories)}\")\n",
        "print(f\"  Distribution: {np.bincount(labels_int)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf31623",
      "metadata": {
        "id": "3bf31623"
      },
      "source": [
        "## 3. Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "759cce74",
      "metadata": {
        "id": "759cce74"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"PREPROCESSING PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Pipeline de pr√©paration (RGB pour Transfer Learning)\n",
        "prep_pipeline = Pipeline([\n",
        "    ('load', ImageLoader(color_mode='RGB', verbose=False)),  # RGB pour ImageNet\n",
        "    ('resize', ImageResizer(img_size=(224, 224), verbose=False)),  # 224x224 pour ImageNet\n",
        "    ('norm', ImageNormalizer(method='minmax', verbose=False))\n",
        "])\n",
        "\n",
        "print(\"\\nChargement des images...\")\n",
        "images = prep_pipeline.fit_transform(image_paths)\n",
        "\n",
        "print(f\"\\nüìä Images pr√©par√©es:\")\n",
        "print(f\"  Shape: {images.shape}\")\n",
        "print(f\"  Range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "print(f\"  Dtype: {images.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71445b3",
      "metadata": {
        "id": "d71445b3"
      },
      "source": [
        "## 4. Train/Validation/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7fa54db",
      "metadata": {
        "id": "b7fa54db"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAIN/VALIDATION/TEST SPLIT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Split 70/15/15\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    images, labels_int,\n",
        "    test_size=0.15,\n",
        "    random_state=config.random_seed,\n",
        "    stratify=labels_int\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.176,  # 0.176 * 0.85 ‚âà 0.15 du total\n",
        "    random_state=config.random_seed,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "\n",
        "# One-hot encoding\n",
        "y_train_cat = keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
        "y_val_cat = keras.utils.to_categorical(y_val, num_classes=len(categories))\n",
        "y_test_cat = keras.utils.to_categorical(y_test, num_classes=len(categories))\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} images\")\n",
        "print(f\"  Distribution: {np.bincount(y_train)}\")\n",
        "print(f\"\\nValidation set: {X_val.shape[0]} images\")\n",
        "print(f\"  Distribution: {np.bincount(y_val)}\")\n",
        "print(f\"\\nTest set: {X_test.shape[0]} images\")\n",
        "print(f\"  Distribution: {np.bincount(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effa80a2",
      "metadata": {
        "id": "effa80a2"
      },
      "source": [
        "## 5. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ce3700",
      "metadata": {
        "id": "24ce3700"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA AUGMENTATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Data augmentation pour le training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Pas d'augmentation pour validation/test\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "print(\"\\n‚úÖ Data augmentation configur√©e\")\n",
        "print(\"   ‚Ä¢ Rotation: ¬±15¬∞\")\n",
        "print(\"   ‚Ä¢ Shift: ¬±10%\")\n",
        "print(\"   ‚Ä¢ Flip horizontal: Oui\")\n",
        "print(\"   ‚Ä¢ Zoom: ¬±10%\")\n",
        "print(\"   ‚Ä¢ Brightness: 0.8-1.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7f7a13",
      "metadata": {
        "id": "2c7f7a13"
      },
      "source": [
        "## 6. Fonction de Cr√©ation de Mod√®le Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86300833",
      "metadata": {
        "id": "86300833"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0, InceptionV3\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_transfer_model(base_model_name='VGG16', freeze_base=True, trainable_layers=0):\n",
        "    \"\"\"\n",
        "    Cr√©e un mod√®le de Transfer Learning\n",
        "\n",
        "    Args:\n",
        "        base_model_name: 'VGG16', 'ResNet50', 'EfficientNetB0', 'InceptionV3'\n",
        "        freeze_base: Si True, freeze les poids du base model\n",
        "        trainable_layers: Nombre de layers √† rendre trainable (si freeze_base=False)\n",
        "    \"\"\"\n",
        "    input_shape = (224, 224, 3)\n",
        "\n",
        "    # S√©lectionner le base model\n",
        "    if base_model_name == 'VGG16':\n",
        "        base_model = VGG16(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "    elif base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "    elif base_model_name == 'EfficientNetB0':\n",
        "        base_model = EfficientNetB0(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "    elif base_model_name == 'InceptionV3':\n",
        "        base_model = InceptionV3(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Base model inconnu: {base_model_name}\")\n",
        "\n",
        "    # Freeze/Unfreeze\n",
        "    if freeze_base:\n",
        "        base_model.trainable = False\n",
        "    else:\n",
        "        # Unfreeze les derni√®res couches\n",
        "        for layer in base_model.layers[:-trainable_layers]:\n",
        "            layer.trainable = False\n",
        "        for layer in base_model.layers[-trainable_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "    # Construction du mod√®le\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(len(categories), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "print(\"‚úÖ Fonction de cr√©ation de mod√®le d√©finie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d68c94a",
      "metadata": {
        "id": "6d68c94a"
      },
      "source": [
        "## 7. Phase 1 - Feature Extraction (VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a39c8c33",
      "metadata": {
        "id": "a39c8c33"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"PHASE 1: FEATURE EXTRACTION - VGG16\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Cr√©er le mod√®le avec base freez√©\n",
        "model_vgg16, base_vgg16 = create_transfer_model('VGG16', freeze_base=True)\n",
        "\n",
        "# Compilation\n",
        "model_vgg16.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "print(\"\\nüìê Architecture:\")\n",
        "model_vgg16.summary()\n",
        "\n",
        "# Compter les param√®tres\n",
        "trainable_params = sum([tf.size(w).numpy() for w in model_vgg16.trainable_weights])\n",
        "total_params = sum([tf.size(w).numpy() for w in model_vgg16.weights])\n",
        "print(f\"\\nParam√®tres trainable: {trainable_params:,}\")\n",
        "print(f\"Param√®tres total: {total_params:,}\")\n",
        "print(f\"Ratio trainable: {trainable_params/total_params:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adafa55",
      "metadata": {
        "id": "0adafa55"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks_vgg16 = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=config.early_stopping_patience,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=config.reduce_lr_factor,\n",
        "        patience=config.reduce_lr_patience,\n",
        "        min_lr=config.min_lr,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(config.models_dir / 'vgg16_feature_extraction_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Entra√Ænement\n",
        "print(\"\\nüöÄ Entra√Ænement Phase 1 (Feature Extraction)...\\n\")\n",
        "\n",
        "history_vgg16_fe = model_vgg16.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=30,  # Moins d'√©poques pour feature extraction\n",
        "    callbacks=callbacks_vgg16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Phase 1 termin√©e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d529680",
      "metadata": {
        "id": "3d529680"
      },
      "source": [
        "## 8. Phase 2 - Fine-Tuning (VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d977c7",
      "metadata": {
        "id": "e2d977c7"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"PHASE 2: FINE-TUNING - VGG16\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Unfreeze les derni√®res 4 couches du base model\n",
        "base_vgg16.trainable = True\n",
        "for layer in base_vgg16.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(f\"\\nLayers trainable: {sum([1 for l in base_vgg16.layers if l.trainable])}\")\n",
        "print(f\"Layers frozen: {sum([1 for l in base_vgg16.layers if not l.trainable])}\")\n",
        "\n",
        "# Recompiler avec learning rate plus faible\n",
        "model_vgg16.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # 10x plus faible\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Nouveaux callbacks\n",
        "callbacks_vgg16_ft = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(config.models_dir / 'vgg16_finetuned_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Fine-tuning\n",
        "print(\"\\nüöÄ Entra√Ænement Phase 2 (Fine-Tuning)...\\n\")\n",
        "\n",
        "history_vgg16_ft = model_vgg16.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=50,\n",
        "    callbacks=callbacks_vgg16_ft,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Phase 2 termin√©e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3a3e27",
      "metadata": {
        "id": "af3a3e27"
      },
      "source": [
        "## 9. ResNet50 (Feature Extraction + Fine-Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3435a98d",
      "metadata": {
        "id": "3435a98d"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RESNET50 - FEATURE EXTRACTION + FINE-TUNING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Phase 1: Feature Extraction\n",
        "model_resnet, base_resnet = create_transfer_model('ResNet50', freeze_base=True)\n",
        "\n",
        "model_resnet.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "callbacks_resnet = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(config.models_dir / 'resnet50_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Phase 1: Feature Extraction\\n\")\n",
        "history_resnet_fe = model_resnet.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=30,\n",
        "    callbacks=callbacks_resnet,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Phase 2: Fine-Tuning\n",
        "print(\"\\nüöÄ Phase 2: Fine-Tuning\\n\")\n",
        "base_resnet.trainable = True\n",
        "for layer in base_resnet.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_resnet.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "history_resnet_ft = model_resnet.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=50,\n",
        "    callbacks=callbacks_resnet,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ ResNet50 termin√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ff449f",
      "metadata": {
        "id": "34ff449f"
      },
      "source": [
        "## 10. EfficientNetB0 (Feature Extraction + Fine-Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8add842",
      "metadata": {
        "id": "c8add842"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EFFICIENTNETB0 - FEATURE EXTRACTION + FINE-TUNING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Phase 1: Feature Extraction\n",
        "model_effnet, base_effnet = create_transfer_model('EfficientNetB0', freeze_base=True)\n",
        "\n",
        "model_effnet.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "callbacks_effnet = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(config.models_dir / 'efficientnetb0_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Phase 1: Feature Extraction\\n\")\n",
        "history_effnet_fe = model_effnet.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=30,\n",
        "    callbacks=callbacks_effnet,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Phase 2: Fine-Tuning\n",
        "print(\"\\nüöÄ Phase 2: Fine-Tuning\\n\")\n",
        "base_effnet.trainable = True\n",
        "for layer in base_effnet.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_effnet.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "history_effnet_ft = model_effnet.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=50,\n",
        "    callbacks=callbacks_effnet,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ EfficientNetB0 termin√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9057874",
      "metadata": {},
      "source": [
        "## 11. InceptionV3 (Feature Extraction + Fine-Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec2ad2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"INCEPTIONV3 - FEATURE EXTRACTION + FINE-TUNING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Phase 1: Feature Extraction\n",
        "model_inception, base_inception = create_transfer_model('InceptionV3', freeze_base=True)\n",
        "\n",
        "model_inception.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "callbacks_inception = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(config.models_dir / 'inceptionv3_best.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Phase 1: Feature Extraction\\n\")\n",
        "history_inception_fe = model_inception.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=30,\n",
        "    callbacks=callbacks_inception,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Phase 2: Fine-Tuning\n",
        "print(\"\\nüöÄ Phase 2: Fine-Tuning\\n\")\n",
        "base_inception.trainable = True\n",
        "for layer in base_inception.layers[:-30]:  # InceptionV3 a beaucoup de layers\n",
        "    layer.trainable = False\n",
        "\n",
        "model_inception.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "history_inception_ft = model_inception.fit(\n",
        "    train_datagen.flow(X_train, y_train_cat, batch_size=config.batch_size),\n",
        "    validation_data=val_datagen.flow(X_val, y_val_cat, batch_size=config.batch_size),\n",
        "    epochs=50,\n",
        "    callbacks=callbacks_inception,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ InceptionV3 termin√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01766444",
      "metadata": {
        "id": "01766444"
      },
      "source": [
        "## 12. √âvaluation sur Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1875f373",
      "metadata": {
        "id": "1875f373"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"√âVALUATION SUR TEST SET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "models_dict = {\n",
        "    'VGG16': model_vgg16,\n",
        "    'ResNet50': model_resnet,\n",
        "    'EfficientNetB0': model_effnet,\n",
        "    'InceptionV3': model_inception\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Pr√©dictions\n",
        "    y_pred_proba = model.predict(X_test, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    # M√©triques\n",
        "    test_loss, test_acc, test_prec, test_rec = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "\n",
        "    results[name] = {\n",
        "        'accuracy': test_acc,\n",
        "        'precision': test_prec,\n",
        "        'recall': test_rec,\n",
        "        'loss': test_loss,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "    print(f\"\\n  Test Accuracy:  {test_acc:.4f}\")\n",
        "    print(f\"  Test Precision: {test_prec:.4f}\")\n",
        "    print(f\"  Test Recall:    {test_rec:.4f}\")\n",
        "    print(f\"  Test Loss:      {test_loss:.4f}\")\n",
        "\n",
        "    # Rapport de classification\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=categories, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e153a2e",
      "metadata": {
        "id": "8e153a2e"
      },
      "source": [
        "## 13. Comparaison des Mod√®les"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85115f6",
      "metadata": {
        "id": "e85115f6"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"COMPARAISON DES MOD√àLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Tableau comparatif\n",
        "print(\"\\n{:<20s} {:>12s} {:>12s} {:>12s} {:>12s}\".format(\n",
        "    'Mod√®le', 'Accuracy', 'Precision', 'Recall', 'Loss'\n",
        "))\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for name, res in sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True):\n",
        "    print(\"{:<20s} {:>12.4f} {:>12.4f} {:>12.4f} {:>12.4f}\".format(\n",
        "        name,\n",
        "        res['accuracy'],\n",
        "        res['precision'],\n",
        "        res['recall'],\n",
        "        res['loss']\n",
        "    ))\n",
        "\n",
        "# Meilleur mod√®le\n",
        "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
        "best_acc = results[best_model_name]['accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üèÜ MEILLEUR MOD√àLE: {best_model_name} (Accuracy: {best_acc:.4f})\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3b9bc3",
      "metadata": {
        "id": "fb3b9bc3"
      },
      "source": [
        "## 14. Courbes d'Apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6eb041",
      "metadata": {
        "id": "7c6eb041"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 2, figsize=(16, 22))\n",
        "\n",
        "histories = {\n",
        "    'VGG16': [history_vgg16_fe, history_vgg16_ft],\n",
        "    'ResNet50': [history_resnet_fe, history_resnet_ft],\n",
        "    'EfficientNetB0': [history_effnet_fe, history_effnet_ft],\n",
        "    'InceptionV3': [history_inception_fe, history_inception_ft]\n",
        "}\n",
        "\n",
        "for idx, (name, hists) in enumerate(histories.items()):\n",
        "    # Accuracy\n",
        "    ax_acc = axes[idx, 0]\n",
        "    for i, h in enumerate(hists):\n",
        "        phase = \"FE\" if i == 0 else \"FT\"\n",
        "        ax_acc.plot(h.history['accuracy'], label=f'{phase} Train', linewidth=2)\n",
        "        ax_acc.plot(h.history['val_accuracy'], label=f'{phase} Val', linewidth=2, linestyle='--')\n",
        "\n",
        "    ax_acc.set_xlabel('Epoch', fontsize=11)\n",
        "    ax_acc.set_ylabel('Accuracy', fontsize=11)\n",
        "    ax_acc.set_title(f'{name} - Accuracy', fontsize=12, weight='bold')\n",
        "    ax_acc.legend(fontsize=9)\n",
        "    ax_acc.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss\n",
        "    ax_loss = axes[idx, 1]\n",
        "    for i, h in enumerate(hists):\n",
        "        phase = \"FE\" if i == 0 else \"FT\"\n",
        "        ax_loss.plot(h.history['loss'], label=f'{phase} Train', linewidth=2)\n",
        "        ax_loss.plot(h.history['val_loss'], label=f'{phase} Val', linewidth=2, linestyle='--')\n",
        "\n",
        "    ax_loss.set_xlabel('Epoch', fontsize=11)\n",
        "    ax_loss.set_ylabel('Loss', fontsize=11)\n",
        "    ax_loss.set_title(f'{name} - Loss', fontsize=12, weight='bold')\n",
        "    ax_loss.legend(fontsize=9)\n",
        "    ax_loss.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Courbes d\\'Apprentissage - Transfer Learning', fontsize=14, weight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd6085",
      "metadata": {
        "id": "25dd6085"
      },
      "source": [
        "## 15. Matrices de Confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a13ac92",
      "metadata": {
        "id": "8a13ac92"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, (name, res) in zip(axes, results.items()):\n",
        "    cm = confusion_matrix(y_test, res['y_pred'])\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=categories, yticklabels=categories,\n",
        "        ax=ax, cbar_kws={'label': 'Count'}\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'{name}\\nAcc: {res[\"accuracy\"]:.3f}', fontsize=11, weight='bold')\n",
        "    ax.set_xlabel('Predicted', fontsize=10)\n",
        "    ax.set_ylabel('True', fontsize=10)\n",
        "\n",
        "plt.suptitle('Matrices de Confusion - Test Set', fontsize=13, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6178374a",
      "metadata": {
        "id": "6178374a"
      },
      "source": [
        "## 16. Sauvegarde des R√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52119836",
      "metadata": {
        "id": "52119836"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Cr√©er un r√©sum√©\n",
        "summary = {\n",
        "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'dataset': {\n",
        "        'total_images': len(image_paths),\n",
        "        'train': len(X_train),\n",
        "        'val': len(X_val),\n",
        "        'test': len(X_test),\n",
        "        'classes': categories\n",
        "    },\n",
        "    'models': {}\n",
        "}\n",
        "\n",
        "for name, res in results.items():\n",
        "    summary['models'][name] = {\n",
        "        'accuracy': float(res['accuracy']),\n",
        "        'precision': float(res['precision']),\n",
        "        'recall': float(res['recall']),\n",
        "        'loss': float(res['loss'])\n",
        "    }\n",
        "\n",
        "summary['best_model'] = best_model_name\n",
        "\n",
        "# Sauvegarder\n",
        "results_file = config.results_dir / f'transfer_learning_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ R√©sultats sauvegard√©s: {results_file}\")\n",
        "print(f\"\\nüìä R√©sum√©:\")\n",
        "print(json.dumps(summary, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea721bc",
      "metadata": {
        "id": "4ea721bc"
      },
      "source": [
        "## 17. Conclusion\n",
        "\n",
        "### üìä R√©sultats obtenus\n",
        "\n",
        "Vous avez test√© 4 architectures de Transfer Learning :\n",
        "- **VGG16** : Feature extraction + Fine-tuning\n",
        "- **ResNet50** : Feature extraction + Fine-tuning  \n",
        "- **EfficientNetB0** : Feature extraction + Fine-tuning\n",
        "- **InceptionV3** : Feature extraction + Fine-tuning\n",
        "\n",
        "### üéØ Prochaines √©tapes\n",
        "\n",
        "1. **Interpr√©tabilit√©** : GradCAM sur le meilleur mod√®le\n",
        "2. **Ensemble Methods** : Combiner les 4 mod√®les (voting)\n",
        "3. **Hyperparameter Tuning** : Tester diff√©rentes architectures de head\n",
        "4. **More Data** : Augmenter le dataset si possible\n",
        "5. **Cross-Validation** : Valider la robustesse des r√©sultats\n",
        "\n",
        "### üíæ Mod√®les sauvegard√©s\n",
        "\n",
        "Les meilleurs mod√®les ont √©t√© sauvegard√©s dans `models/` :\n",
        "- `vgg16_finetuned_best.keras`\n",
        "- `resnet50_best.keras`\n",
        "- `efficientnetb0_best.keras`\n",
        "- `inceptionv3_best.keras`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
