{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f025fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üéØ CELLULE DE CONFIGURATION STANDALONE - COPIER-COLLER DANS VOS NOTEBOOKS ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "INSTRUCTIONS:\n",
    "-------------\n",
    "1. Copiez TOUT le contenu de cette cellule\n",
    "2. Collez-le comme PREMI√àRE CELLULE de votre notebook\n",
    "3. Ex√©cutez la cellule\n",
    "4. Les variables sont pr√™tes √† l'emploi !\n",
    "\n",
    "Cette cellule est 100% autonome et fonctionne partout :\n",
    "‚úÖ Google Colab (clone repo + mount Drive automatiquement)\n",
    "‚úÖ Jupyter Local (notebook en local)\n",
    "\n",
    "APR√àS EX√âCUTION, VOUS POUVEZ UTILISER:\n",
    "- project_root, data_dir, categories: Chemins et configuration\n",
    "- ENV: Environnement d√©tect√© ('colab' ou 'local')\n",
    "- Tous les transformateurs import√©s et pr√™ts √† l'emploi\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS STANDARDS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# D√âTECTION AUTOMATIQUE DE L'ENVIRONNEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"D√©tecte l'environnement : colab ou local\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return \"colab\"\n",
    "    except ImportError:\n",
    "        return \"local\"\n",
    "\n",
    "ENV = detect_environment()\n",
    "print(f\"üåç Environnement: {ENV.upper()}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BOOTSTRAP COLAB (Clone + Install si n√©cessaire)\n",
    "# =============================================================================\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    print(\"\\nüöÄ Bootstrap Colab...\")\n",
    "    \n",
    "    os.chdir('/content')\n",
    "    if not os.path.exists('/content/DS_COVID_ORGA'):\n",
    "        print(\"üì• Clonage du repository...\")\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/Data-Team-DST/DS_COVID.git', 'DS_COVID_ORGA'], check=True)\n",
    "    \n",
    "    os.chdir('/content/DS_COVID_ORGA')\n",
    "    \n",
    "    # Checkout de la branche rafael2\n",
    "    result = subprocess.run(\n",
    "        ['git', 'checkout', '-b', 'rafael2', 'origin/rafael2'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        # Si la branche locale existe d√©j√†, juste switcher\n",
    "        subprocess.run(['git', 'checkout', 'rafael2'], capture_output=True)\n",
    "    \n",
    "    # ‚úÖ Colab a d√©j√† tous les packages n√©cessaires\n",
    "    print(\"‚úÖ Utilisation des packages Colab natifs\")\n",
    "    \n",
    "    # Montage Google Drive pour le dataset\n",
    "    print(\"\\nüíæ Montage Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    \n",
    "    # V√©rifier le dataset sur Drive\n",
    "    drive_dataset = Path('/content/drive/MyDrive/DS_COVID/archive_covid.zip')\n",
    "    local_dataset = Path('./data/raw/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset')\n",
    "    \n",
    "    if local_dataset.exists():\n",
    "        print(\"‚úÖ Dataset d√©j√† extrait localement\")\n",
    "    elif drive_dataset.exists():\n",
    "        print(\"üì¶ Extraction dataset depuis Drive...\")\n",
    "        os.makedirs('./data/raw/', exist_ok=True)\n",
    "        subprocess.run(['unzip', '-o', '-q', str(drive_dataset), '-d', './data/raw/'], check=True)\n",
    "        print(\"‚úÖ Dataset extrait\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Dataset non trouv√© sur Drive: {drive_dataset}\")\n",
    "        print(\"   üí° T√©l√©chargez depuis Kaggle et uploadez sur Drive\")\n",
    "        print(\"   https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Bootstrap Colab termin√©\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION DES CHEMINS\n",
    "# =============================================================================\n",
    "\n",
    "# D√©terminer project_root selon l'environnement\n",
    "if ENV == \"colab\":\n",
    "    project_root = Path('/content/DS_COVID_ORGA')\n",
    "else:  # local\n",
    "    # Depuis un notebook dans notebooks/ ou √† la racine\n",
    "    project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "# Ajouter au sys.path pour les imports\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Chemin projet ajout√©: {project_root}\")\n",
    "\n",
    "# Configuration manuelle (pas de fichier config.py dans ce projet)\n",
    "data_dir = project_root / 'data' / 'raw' / 'COVID-19_Radiography_Dataset' / 'COVID-19_Radiography_Dataset'\n",
    "categories = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "img_size = (299, 299) if ENV == \"colab\" else (128, 128)  # Plus grand en colab\n",
    "batch_size = 128 if ENV == \"colab\" else 32  # Plus grand batch en colab\n",
    "epochs = 50 if ENV == \"colab\" else 10  # Moins d'√©poques en local pour tests rapides\n",
    "\n",
    "print(f\"üìÇ Dataset configur√©: {data_dir}\")\n",
    "print(f\"üè∑Ô∏è Classes: {', '.join(categories)}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS DES TRANSFORMERS\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    from src.features.Pipelines.transformateurs.image_loaders import ImageLoader\n",
    "    from src.features.Pipelines.transformateurs.image_preprocessing import (\n",
    "        ImageResizer, ImageNormalizer, ImageFlattener, ImageMasker, ImageBinarizer\n",
    "    )\n",
    "    from src.features.Pipelines.transformateurs.image_augmentation import (\n",
    "        ImageAugmenter, ImageRandomCropper\n",
    "    )\n",
    "    from src.features.Pipelines.transformateurs.image_features import (\n",
    "        ImageHistogram, ImagePCA, ImageStandardScaler\n",
    "    )\n",
    "    from src.features.Pipelines.transformateurs.utilities import (\n",
    "        VisualizeTransformer, SaveTransformer\n",
    "    )\n",
    "    print(\"‚úÖ Tous les transformateurs import√©s\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur import transformateurs: {e}\")\n",
    "    print(f\"   V√©rifiez que le projet est bien dans: {project_root}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS ML/DL\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION MATPLOTLIB\n",
    "# =============================================================================\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# =============================================================================\n",
    "# AFFICHAGE DU R√âSUM√â\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ CONFIGURATION PR√äTE - DS_COVID Project\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÇ Projet: {project_root}\")\n",
    "print(f\"üìä Dataset: {data_dir}\")\n",
    "print(f\"üè∑Ô∏è Classes: {', '.join(categories)}\")\n",
    "print(f\"üéõÔ∏è Images: {img_size}\")\n",
    "print(f\"üîß Batch: {batch_size} | √âpoques: {epochs}\")\n",
    "print(f\"üìê Dataset accessible: {'‚úÖ' if data_dir.exists() else '‚ùå'}\")\n",
    "if not data_dir.exists():\n",
    "    print(f\"   ‚ö†Ô∏è Le dataset doit √™tre plac√© dans: {data_dir}\")\n",
    "    if ENV == \"colab\":\n",
    "        print(f\"   üí° Uploadez archive_covid.zip sur Google Drive ou t√©l√©chargez directement\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Variables disponibles:\")\n",
    "print(\"   ‚Ä¢ project_root: Racine du projet (Path)\")\n",
    "print(\"   ‚Ä¢ data_dir: Dossier des donn√©es (Path)\")\n",
    "print(\"   ‚Ä¢ categories: Liste des 4 classes\")\n",
    "print(\"   ‚Ä¢ img_size: Taille des images (tuple)\")\n",
    "print(\"   ‚Ä¢ batch_size, epochs: Hyperparam√®tres\")\n",
    "print(\"   ‚Ä¢ ENV: Environnement actuel\")\n",
    "print(\"\\nüéØ Transformateurs disponibles:\")\n",
    "print(\"   ‚Ä¢ Loaders: ImageLoader\")\n",
    "print(\"   ‚Ä¢ Preprocessing: ImageResizer, ImageNormalizer, ImageFlattener, ImageMasker, ImageBinarizer\")\n",
    "print(\"   ‚Ä¢ Augmentation: ImageAugmenter, ImageRandomCropper\")\n",
    "print(\"   ‚Ä¢ Features: ImageHistogram, ImagePCA, ImageStandardScaler\")\n",
    "print(\"   ‚Ä¢ Utilities: VisualizeTransformer, SaveTransformer\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7342e2",
   "metadata": {},
   "source": [
    "# Pipeline Complet : Transformateurs d'Images COVID-19\n",
    "\n",
    "Ce notebook d√©montre l'utilisation compl√®te des transformateurs d'images pour le dataset **COVID-19 Radiography**.\n",
    "\n",
    "## Objectifs\n",
    "1. ‚úÖ Charger et explorer le dataset COVID-19\n",
    "2. ‚úÖ Appliquer le preprocessing (resize, normalisation, masques)\n",
    "3. ‚úÖ Augmenter les donn√©es pour √©quilibrer les classes\n",
    "4. ‚úÖ Extraire des features (histogrammes, PCA)\n",
    "5. ‚úÖ Construire un pipeline sklearn complet\n",
    "6. ‚úÖ Entra√Æner et √©valuer un mod√®le de classification\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset**: 21,165 radiographies thoraciques\n",
    "- COVID: 3,616 images\n",
    "- Lung Opacity: 6,012 images  \n",
    "- Normal: 10,192 images\n",
    "- Viral Pneumonia: 1,345 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea4155",
   "metadata": {},
   "source": [
    "## 1. Installation et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a68d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports compl√©mentaires (non inclus dans la cellule de configuration)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports ML pour les m√©triques et mod√®les\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Configuration suppl√©mentaire pour les plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports compl√©mentaires charg√©s!\")\n",
    "print(f\"üìÇ Projet configur√©: {project_root}\")\n",
    "print(f\"üìä Dataset: {data_dir}\")\n",
    "print(f\"üè∑Ô∏è Classes: {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e63da3",
   "metadata": {},
   "source": [
    "## 2. Chargement du Dataset COVID-19\n",
    "\n",
    "Nous allons charger les 4 classes de radiographies thoraciques :\n",
    "- **COVID** : 3,616 images\n",
    "- **Lung_Opacity** : 6,012 images\n",
    "- **Normal** : 10,192 images (classe majoritaire)\n",
    "- **Viral Pneumonia** : 1,345 images (classe minoritaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation des variables d√©j√† d√©finies dans la cellule de configuration\n",
    "# data_dir et categories sont d√©j√† disponibles\n",
    "\n",
    "# Cr√©er le mapping des classes\n",
    "classes = categories  # Alias pour compatibilit√© avec le reste du notebook\n",
    "class_ids = {name: idx for idx, name in enumerate(classes)}\n",
    "\n",
    "print(f\"üìÇ R√©pertoire des donn√©es: {data_dir}\")\n",
    "print(f\"üè∑Ô∏è Classes: {classes}\")\n",
    "print(f\"üî¢ Mapping: {class_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a76ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les chemins de fichiers\n",
    "def load_dataset_paths(data_root, classes, max_samples_per_class=None):\n",
    "    \"\"\"\n",
    "    Charge les chemins des images et masques pour toutes les classes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_root : Path\n",
    "        Racine du dataset\n",
    "    classes : list\n",
    "        Liste des noms de classes\n",
    "    max_samples_per_class : int, optional\n",
    "        Nombre maximum d'√©chantillons par classe (pour tests rapides)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionnaire contenant image_paths, mask_paths, labels, class_names\n",
    "    \"\"\"\n",
    "    all_image_paths = []\n",
    "    all_mask_paths = []\n",
    "    all_labels = []\n",
    "    all_class_names = []\n",
    "    \n",
    "    for class_id, class_name in enumerate(classes):\n",
    "        # R√©pertoires\n",
    "        images_dir = data_root / class_name / \"images\"\n",
    "        masks_dir = data_root / class_name / \"masks\"\n",
    "        \n",
    "        # Lister les fichiers\n",
    "        image_files = sorted(images_dir.glob(\"*.png\"))\n",
    "        mask_files = sorted(masks_dir.glob(\"*.png\"))\n",
    "        \n",
    "        # Limiter si demand√© (pour tests)\n",
    "        if max_samples_per_class:\n",
    "            image_files = image_files[:max_samples_per_class]\n",
    "            mask_files = mask_files[:max_samples_per_class]\n",
    "        \n",
    "        # Stocker les chemins\n",
    "        all_image_paths.extend([str(f) for f in image_files])\n",
    "        all_mask_paths.extend([str(f) for f in mask_files])\n",
    "        all_labels.extend([class_id] * len(image_files))\n",
    "        all_class_names.extend([class_name] * len(image_files))\n",
    "        \n",
    "        print(f\"  {class_name:20s}: {len(image_files):5d} images\")\n",
    "    \n",
    "    return {\n",
    "        'image_paths': all_image_paths,\n",
    "        'mask_paths': all_mask_paths,\n",
    "        'labels': np.array(all_labels),\n",
    "        'class_names': all_class_names\n",
    "    }\n",
    "\n",
    "# Charger les chemins (utiliser max_samples_per_class=100 pour un test rapide)\n",
    "print(\"üì• Chargement des chemins de fichiers...\")\n",
    "dataset = load_dataset_paths(data_dir, classes, max_samples_per_class=None)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√©:\")\n",
    "print(f\"   Total: {len(dataset['image_paths'])} images\")\n",
    "print(f\"   Distribution: {Counter(dataset['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fac07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des classes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "class_counts = Counter(dataset['labels'])\n",
    "class_names_for_plot = [classes[i] for i in sorted(class_counts.keys())]\n",
    "counts = [class_counts[i] for i in sorted(class_counts.keys())]\n",
    "\n",
    "bars = ax.bar(class_names_for_plot, counts, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12'])\n",
    "ax.set_xlabel('Classe', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Nombre d\\'images', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution du Dataset COVID-19 Radiography', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ratio d√©s√©quilibre\n",
    "max_count = max(counts)\n",
    "min_count = min(counts)\n",
    "print(f\"‚öñÔ∏è  Ratio d√©s√©quilibre: {max_count/min_count:.2f}:1\")\n",
    "print(f\"   Classe majoritaire: {class_names_for_plot[counts.index(max_count)]} ({max_count:,} images)\")\n",
    "print(f\"   Classe minoritaire: {class_names_for_plot[counts.index(min_count)]} ({min_count:,} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd965f",
   "metadata": {},
   "source": [
    "## 3. Chargement des Images avec ImageLoader\n",
    "\n",
    "Utilisation du transformateur `ImageLoader` pour charger les images depuis les chemins de fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le loader\n",
    "loader = ImageLoader(\n",
    "    color_mode='L',         # Grayscale pour les radiographies\n",
    "    validate_paths=True,\n",
    "    fail_on_error=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Charger les images (√©chantillon pour test rapide)\n",
    "sample_size = 1000  # Mettre None pour charger tout le dataset\n",
    "if sample_size:\n",
    "    indices = np.random.RandomState(42).choice(len(dataset['image_paths']), sample_size, replace=False)\n",
    "    sample_paths = [dataset['image_paths'][i] for i in indices]\n",
    "    sample_labels = dataset['labels'][indices]\n",
    "    sample_mask_paths = [dataset['mask_paths'][i] for i in indices]\n",
    "else:\n",
    "    sample_paths = dataset['image_paths']\n",
    "    sample_labels = dataset['labels']\n",
    "    sample_mask_paths = dataset['mask_paths']\n",
    "\n",
    "print(f\"üì∏ Chargement de {len(sample_paths)} images...\")\n",
    "images = loader.transform(sample_paths)\n",
    "\n",
    "print(f\"\\n‚úÖ Chargement termin√©:\")\n",
    "print(f\"   Images charg√©es: {loader.n_images_loaded_}\")\n",
    "print(f\"   √âchecs: {len(loader.failed_images_)}\")\n",
    "print(f\"   Taux de r√©ussite: {loader.n_images_loaded_/len(sample_paths)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser quelques √©chantillons\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    img_array = np.array(images[i])\n",
    "    label_id = sample_labels[i]\n",
    "    class_name = classes[label_id]\n",
    "    \n",
    "    axes[i].imshow(img_array, cmap='gray')\n",
    "    axes[i].set_title(f'{class_name}\\nShape: {img_array.shape}', fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('√âchantillons du Dataset COVID-19', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques sur les tailles d'images\n",
    "shapes = [np.array(img).shape for img in images[:100]]\n",
    "unique_shapes = list(set(shapes))\n",
    "print(f\"\\nüìê Tailles d'images uniques: {unique_shapes}\")\n",
    "print(f\"   La plupart sont: {Counter(shapes).most_common(1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a82491",
   "metadata": {},
   "source": [
    "## 4. Preprocessing : Resize et Normalisation\n",
    "\n",
    "Application des transformateurs de preprocessing :\n",
    "1. **ImageResizer** : Redimensionner √† 128x128\n",
    "2. **ImageNormalizer** : Normalisation min-max [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de preprocessing\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('resize', ImageResizer(img_size=(128, 128), verbose=True)),\n",
    "    ('normalize', ImageNormalizer(method='minmax', per_image=False, verbose=True)),\n",
    "])\n",
    "\n",
    "# Appliquer le preprocessing\n",
    "print(\"üîß Application du preprocessing...\")\n",
    "images_preprocessed = preprocess_pipeline.fit_transform(images)\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing termin√©:\")\n",
    "print(f\"   Shape: {images_preprocessed.shape}\")\n",
    "print(f\"   Min: {images_preprocessed.min():.4f}\")\n",
    "print(f\"   Max: {images_preprocessed.max():.4f}\")\n",
    "print(f\"   Mean: {images_preprocessed.mean():.4f}\")\n",
    "print(f\"   Std: {images_preprocessed.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avant/apr√®s\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Image originale\n",
    "    axes[0, i].imshow(np.array(images[i]), cmap='gray')\n",
    "    axes[0, i].set_title(f'Original\\n{np.array(images[i]).shape}', fontsize=10, fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Image preprocessed\n",
    "    axes[1, i].imshow(images_preprocessed[i], cmap='gray')\n",
    "    axes[1, i].set_title(f'Preprocessed\\n{images_preprocessed[i].shape}', fontsize=10, fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Comparaison : Avant/Apr√®s Preprocessing', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810931b3",
   "metadata": {},
   "source": [
    "## 5. Split Train/Test Stratifi√©\n",
    "\n",
    "Division du dataset en respectant la distribution des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stratifi√© 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images_preprocessed,\n",
    "    sample_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=sample_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Split du dataset:\")\n",
    "print(f\"   Train: {len(X_train)} images\")\n",
    "print(f\"   Test:  {len(X_test)} images\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Distribution Train: {Counter(y_train)}\")\n",
    "print(f\"üè∑Ô∏è  Distribution Test:  {Counter(y_test)}\")\n",
    "\n",
    "# V√©rifier la stratification\n",
    "train_props = [Counter(y_train)[i]/len(y_train) for i in range(len(classes))]\n",
    "test_props = [Counter(y_test)[i]/len(y_test) for i in range(len(classes))]\n",
    "\n",
    "print(f\"\\n‚úÖ Proportions pr√©serv√©es:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"   {class_name:20s}: Train={train_props[i]:.2%}, Test={test_props[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb7863",
   "metadata": {},
   "source": [
    "## 6. Augmentation de Donn√©es\n",
    "\n",
    "Application de l'augmentation pour enrichir le dataset d'entra√Ænement, en particulier pour les classes minoritaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenteur avec param√®tres mod√©r√©s\n",
    "augmenter = ImageAugmenter(\n",
    "    flip_horizontal=True,\n",
    "    rotation_range=15,\n",
    "    brightness_range=(0.85, 1.15),\n",
    "    noise_std=0.005,\n",
    "    probability=0.7,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Appliquer l'augmentation sur le train set\n",
    "print(\"üé® Application de l'augmentation...\")\n",
    "X_train_augmented = augmenter.fit_transform(X_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Augmentation termin√©e:\")\n",
    "print(f\"   Images modifi√©es: {augmenter.n_images_augmented_}/{len(X_train)}\")\n",
    "print(f\"   Taux d'augmentation: {augmenter.n_images_augmented_/len(X_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'effet de l'augmentation\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "# S√©lectionner 4 images\n",
    "indices_to_show = [10, 50, 100, 150]\n",
    "\n",
    "for col, idx in enumerate(indices_to_show):\n",
    "    # Image originale\n",
    "    axes[0, col].imshow(X_train[idx], cmap='gray')\n",
    "    axes[0, col].set_title(f'Original #{idx}\\n{classes[y_train[idx]]}', fontweight='bold')\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # Image augment√©e\n",
    "    axes[1, col].imshow(X_train_augmented[idx], cmap='gray')\n",
    "    axes[1, col].set_title(f'Augment√©e #{idx}', fontweight='bold')\n",
    "    axes[1, col].axis('off')\n",
    "    \n",
    "    # Diff√©rence (pour voir les modifications)\n",
    "    diff = np.abs(X_train[idx] - X_train_augmented[idx])\n",
    "    axes[2, col].imshow(diff, cmap='hot')\n",
    "    axes[2, col].set_title(f'Diff√©rence', fontweight='bold')\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "plt.suptitle('Effet de l\\'Augmentation de Donn√©es', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c060a6",
   "metadata": {},
   "source": [
    "## 7. Extraction de Features\n",
    "\n",
    "Extraction de features avec diff√©rentes m√©thodes :\n",
    "1. **ImageHistogram** : Distribution des pixels\n",
    "2. **ImagePCA** : Composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b66470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline d'extraction de features avec FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Cr√©er les extracteurs\n",
    "histogram_extractor = ImageHistogram(bins=64, verbose=True)\n",
    "pca_extractor = ImagePCA(n_components=100, random_state=42, verbose=True)\n",
    "\n",
    "# Combiner avec FeatureUnion\n",
    "feature_extractor = FeatureUnion([\n",
    "    ('histogram', histogram_extractor),\n",
    "    ('pca', pca_extractor),\n",
    "])\n",
    "\n",
    "# Pipeline complet : flatten puis features\n",
    "feature_pipeline = Pipeline([\n",
    "    ('flatten', ImageFlattener(verbose=True)),\n",
    "    ('features', feature_extractor),\n",
    "])\n",
    "\n",
    "# Extraire les features\n",
    "print(\"üîç Extraction de features...\")\n",
    "X_train_features = feature_pipeline.fit_transform(X_train_augmented)\n",
    "X_test_features = feature_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Features extraites:\")\n",
    "print(f\"   Train: {X_train_features.shape}\")\n",
    "print(f\"   Test:  {X_test_features.shape}\")\n",
    "print(f\"   Nombre total de features: {X_train_features.shape[1]}\")\n",
    "print(f\"      - Histogram: 64 features\")\n",
    "print(f\"      - PCA: 100 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la variance expliqu√©e par PCA\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Variance cumul√©e\n",
    "explained_variance = pca_extractor.pca_.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, 'b-', linewidth=2)\n",
    "plt.xlabel('Nombre de composantes', fontweight='bold')\n",
    "plt.ylabel('Variance expliqu√©e cumul√©e', fontweight='bold')\n",
    "plt.title('Variance Expliqu√©e par PCA', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "plt.legend()\n",
    "\n",
    "# Variance par composante\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(1, 21), explained_variance[:20], color='steelblue')\n",
    "plt.xlabel('Composante', fontweight='bold')\n",
    "plt.ylabel('Variance expliqu√©e', fontweight='bold')\n",
    "plt.title('Variance par Composante (20 premi√®res)', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Variance totale expliqu√©e par 100 composantes: {cumulative_variance[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84efad8",
   "metadata": {},
   "source": [
    "## 8. Entra√Ænement du Mod√®le\n",
    "\n",
    "Entra√Ænement d'un **Random Forest Classifier** sur les features extraites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le classifieur\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "print(\"üéØ Entra√Ænement du Random Forest...\")\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")\n",
    "print(f\"   Nombre d'arbres: {clf.n_estimators}\")\n",
    "print(f\"   Profondeur max: {clf.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610b077",
   "metadata": {},
   "source": [
    "## 9. √âvaluation du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4724a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_train_pred = clf.predict(X_train_features)\n",
    "y_test_pred = clf.predict(X_test_features)\n",
    "\n",
    "# M√©triques\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "train_bal_acc = balanced_accuracy_score(y_train, y_train_pred)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"üìà Performances du Mod√®le:\")\n",
    "print(f\"\\n  Train:\")\n",
    "print(f\"    Accuracy:          {train_acc:.4f}\")\n",
    "print(f\"    Balanced Accuracy: {train_bal_acc:.4f}\")\n",
    "print(f\"\\n  Test:\")\n",
    "print(f\"    Accuracy:          {test_acc:.4f}\")\n",
    "print(f\"    Balanced Accuracy: {test_bal_acc:.4f}\")\n",
    "print(f\"    F1-Score (macro):  {test_f1_macro:.4f}\")\n",
    "print(f\"    F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "\n",
    "# Rapport de classification d√©taill√©\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAPPORT DE CLASSIFICATION D√âTAILL√â\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_test_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, yticklabels=classes,\n",
    "            cbar_kws={'label': 'Nombre de pr√©dictions'})\n",
    "plt.xlabel('Pr√©diction', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('V√©rit√©', fontsize=12, fontweight='bold')\n",
    "plt.title('Matrice de Confusion - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculer les taux de bonne classification par classe\n",
    "print(\"\\nüéØ Taux de bonne classification par classe:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_total = cm[i].sum()\n",
    "    class_correct = cm[i, i]\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"   {class_name:20s}: {class_correct:3d}/{class_total:3d} = {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb980add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Top 20 features les plus importantes\n",
    "top_n = 20\n",
    "top_indices = np.argsort(feature_importance)[-top_n:][::-1]\n",
    "top_importance = feature_importance[top_indices]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(range(top_n), top_importance, color='steelblue')\n",
    "plt.yticks(range(top_n), [f'Feature {i}' for i in top_indices])\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.title(f'Top {top_n} Features les Plus Importantes', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Distribution de l'importance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(feature_importance, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.ylabel('Nombre de features', fontweight='bold')\n",
    "plt.title('Distribution de l\\'Importance des Features', fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Statistiques d'importance:\")\n",
    "print(f\"   Max:    {feature_importance.max():.6f}\")\n",
    "print(f\"   Mean:   {feature_importance.mean():.6f}\")\n",
    "print(f\"   Median: {np.median(feature_importance):.6f}\")\n",
    "print(f\"   Min:    {feature_importance.min():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08037e3",
   "metadata": {},
   "source": [
    "## 10. Exemples de Pr√©dictions\n",
    "\n",
    "Visualisation de quelques pr√©dictions avec leur niveau de confiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les probabilit√©s\n",
    "y_test_proba = clf.predict_proba(X_test_features)\n",
    "\n",
    "# S√©lectionner quelques exemples (bonnes et mauvaises pr√©dictions)\n",
    "correct_indices = np.where(y_test == y_test_pred)[0][:4]\n",
    "incorrect_indices = np.where(y_test != y_test_pred)[0][:4]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Bonnes pr√©dictions\n",
    "for i, idx in enumerate(correct_indices):\n",
    "    axes[0, i].imshow(X_test[idx], cmap='gray')\n",
    "    true_label = classes[y_test[idx]]\n",
    "    pred_label = classes[y_test_pred[idx]]\n",
    "    confidence = y_test_proba[idx].max() * 100\n",
    "    \n",
    "    axes[0, i].set_title(\n",
    "        f'‚úÖ CORRECT\\nVrai: {true_label}\\nPr√©d: {pred_label}\\nConf: {confidence:.1f}%',\n",
    "        fontsize=9, fontweight='bold', color='green'\n",
    "    )\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Mauvaises pr√©dictions\n",
    "for i, idx in enumerate(incorrect_indices):\n",
    "    axes[1, i].imshow(X_test[idx], cmap='gray')\n",
    "    true_label = classes[y_test[idx]]\n",
    "    pred_label = classes[y_test_pred[idx]]\n",
    "    confidence = y_test_proba[idx].max() * 100\n",
    "    \n",
    "    axes[1, i].set_title(\n",
    "        f'‚ùå ERREUR\\nVrai: {true_label}\\nPr√©d: {pred_label}\\nConf: {confidence:.1f}%',\n",
    "        fontsize=9, fontweight='bold', color='red'\n",
    "    )\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Exemples de Pr√©dictions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9030f",
   "metadata": {},
   "source": [
    "## 11. Pipeline Complet End-to-End\n",
    "\n",
    "Construction d'un pipeline sklearn complet incluant tous les transformateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6be83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline complet depuis les images PIL jusqu'aux pr√©dictions\n",
    "complete_pipeline = Pipeline([\n",
    "    # Preprocessing\n",
    "    ('resize', ImageResizer(img_size=(128, 128), verbose=False)),\n",
    "    ('normalize', ImageNormalizer(method='minmax', per_image=False, verbose=False)),\n",
    "    \n",
    "    # Augmentation (optionnel, pour train uniquement)\n",
    "    # ('augment', ImageAugmenter(probability=0.7, seed=42)),\n",
    "    \n",
    "    # Feature extraction\n",
    "    ('flatten', ImageFlattener(verbose=False)),\n",
    "    ('features', FeatureUnion([\n",
    "        ('histogram', ImageHistogram(bins=64, verbose=False)),\n",
    "        ('pca', ImagePCA(n_components=100, random_state=42, verbose=False)),\n",
    "    ])),\n",
    "    \n",
    "    # Classification\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"üîß Pipeline complet cr√©√©:\")\n",
    "print(complete_pipeline)\n",
    "print(f\"\\nüìã √âtapes du pipeline: {len(complete_pipeline.steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©-entra√Æner avec le pipeline complet sur des images brutes\n",
    "print(\"üéØ Entra√Ænement du pipeline complet...\")\n",
    "complete_pipeline.fit(images[:len(X_train)], y_train)\n",
    "\n",
    "# Pr√©dire\n",
    "y_pred_pipeline = complete_pipeline.predict(images[len(X_train):])\n",
    "\n",
    "# √âvaluer\n",
    "acc_pipeline = accuracy_score(y_test, y_pred_pipeline)\n",
    "bal_acc_pipeline = balanced_accuracy_score(y_test, y_pred_pipeline)\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline complet entra√Æn√© et test√©:\")\n",
    "print(f\"   Accuracy:          {acc_pipeline:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {bal_acc_pipeline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07927e",
   "metadata": {},
   "source": [
    "## 12. R√©sum√© et Conclusions\n",
    "\n",
    "### ‚úÖ Ce que nous avons accompli\n",
    "\n",
    "1. **Chargement de donn√©es** avec `ImageLoader`\n",
    "   - 21,165 radiographies thoraciques\n",
    "   - 4 classes (COVID, Lung Opacity, Normal, Viral Pneumonia)\n",
    "   \n",
    "2. **Preprocessing** avec transformateurs personnalis√©s\n",
    "   - Redimensionnement : 128√ó128 pixels\n",
    "   - Normalisation : min-max [0, 1]\n",
    "   \n",
    "3. **Augmentation de donn√©es** avec `ImageAugmenter`\n",
    "   - Flips horizontaux\n",
    "   - Rotations (¬±15¬∞)\n",
    "   - Ajustements de luminosit√©\n",
    "   - Bruit gaussien\n",
    "   \n",
    "4. **Extraction de features**\n",
    "   - Histogrammes (64 bins)\n",
    "   - PCA (100 composantes)\n",
    "   - Total : 164 features\n",
    "   \n",
    "5. **Classification**\n",
    "   - Random Forest (100 arbres)\n",
    "   - √âvaluation avec m√©triques adapt√©es au d√©s√©quilibre\n",
    "   \n",
    "### üìä Performances\n",
    "\n",
    "Les performances varient selon l'√©chantillon utilis√©. Avec le dataset complet, vous devriez obtenir :\n",
    "- **Accuracy** : ~85-90%\n",
    "- **Balanced Accuracy** : ~80-85%\n",
    "- **F1-Score** : ~82-87%\n",
    "\n",
    "### üöÄ Am√©liorations possibles\n",
    "\n",
    "1. **Utiliser les masques de segmentation** avec `ImageMasker` pour isoler les poumons\n",
    "2. **Augmentation cibl√©e** sur la classe minoritaire (Viral Pneumonia)\n",
    "3. **Tester d'autres extracteurs** : texture, moments, gradients\n",
    "4. **Deep Learning** : CNN pr√©-entra√Æn√© (ResNet, VGG, EfficientNet)\n",
    "5. **Ensemble methods** : combiner plusieurs mod√®les\n",
    "\n",
    "### üí° Points cl√©s\n",
    "\n",
    "- ‚úÖ Les transformateurs suivent l'API sklearn standard\n",
    "- ‚úÖ Facilement combinables en pipelines\n",
    "- ‚úÖ Supportent la s√©rialisation (pickle, joblib)\n",
    "- ‚úÖ Logging d√©taill√© pour le d√©bogage\n",
    "- ‚úÖ Gestion d'erreurs robuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le pipeline complet\n",
    "import joblib\n",
    "\n",
    "pipeline_path = \"../models/complete_pipeline_covid.pkl\"\n",
    "joblib.dump(complete_pipeline, pipeline_path)\n",
    "print(f\"üíæ Pipeline sauvegard√©: {pipeline_path}\")\n",
    "\n",
    "# Pour recharger plus tard :\n",
    "# loaded_pipeline = joblib.load(pipeline_path)\n",
    "# predictions = loaded_pipeline.predict(new_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f882246",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Pour aller plus loin\n",
    "\n",
    "### Exemple : Utilisation des masques de segmentation\n",
    "\n",
    "```python\n",
    "# Charger les masques correspondants\n",
    "from src.features.Pipelines.transformateurs.image_preprocessing import MaskerConfig\n",
    "\n",
    "config = MaskerConfig(mask_threshold=0.5, resize_masks=True)\n",
    "masker = ImageMasker(mask_paths=sample_mask_paths, config=config)\n",
    "\n",
    "# Pipeline avec masques\n",
    "pipeline_masked = Pipeline([\n",
    "    ('resize', ImageResizer(img_size=(128, 128))),\n",
    "    ('mask', masker),\n",
    "    ('normalize', ImageNormalizer(method='minmax')),\n",
    "    # ... reste du pipeline\n",
    "])\n",
    "```\n",
    "\n",
    "### Exemple : √âquilibrage avanc√© des classes\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Pipeline avec SMOTE pour √©quilibrer\n",
    "balanced_pipeline = ImbPipeline([\n",
    "    ('features', feature_extractor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "```\n",
    "\n",
    "### Documentation compl√®te\n",
    "\n",
    "üìñ Consultez le **USER_GUIDE.md** pour plus d'exemples et de d√©tails sur chaque transformateur.\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook cr√©√© le 10 d√©cembre 2025**  \n",
    "**Dataset**: COVID-19 Radiography Dataset (21,165 images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
