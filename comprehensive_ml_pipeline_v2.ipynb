{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L-Poca/Data_Pipeline/blob/rafael_cleaning/notebooks/comprehensive_ml_pipeline_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUL1PqmbHNI0"
      },
      "source": [
        "# ü¶† Comprehensive Machine Learning Pipeline - COVID-19 Classification\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "This notebook provides a **complete machine learning and deep learning pipeline** for COVID-19 classification from chest X-ray images.\n",
        "\n",
        "### üéØ Objectives\n",
        "\n",
        "1. **Baseline ML Models**: Train 9 classical ML algorithms with 3 different feature extraction methods\n",
        "2. **Deep Learning**: Build custom CNNs and leverage 12+ pre-trained architectures\n",
        "3. **Advanced Techniques**: Ensemble methods, hyperparameter tuning, cross-validation\n",
        "4. **Interpretability**: GradCAM, LIME, SHAP explanations\n",
        "5. **Production-Ready**: Model persistence, prediction pipelines, HTML reports\n",
        "\n",
        "### üìä Dataset\n",
        "\n",
        "- **Classes**: COVID, Lung_Opacity, Normal, Viral Pneumonia\n",
        "- **Images**: ~21,000 chest X-rays (grayscale, 256√ó256)\n",
        "- **Challenge**: Class imbalance (Viral Pneumonia: 6.4%)\n",
        "\n",
        "### üöÄ Quick Start\n",
        "\n",
        "1. **Fast Testing**: Set `N_IMAGES_PER_CLASS = 100` (Section 2)\n",
        "2. **Full Training**: Set `N_IMAGES_PER_CLASS = None` (all images)\n",
        "3. **Colab**: Click badge above ‚Üí Auto-clone ‚Üí Auto-install ‚Üí Run all\n",
        "\n",
        "### ‚è±Ô∏è Estimated Runtime\n",
        "\n",
        "- **Fast mode** (100 images/class): ~30-60 minutes\n",
        "- **Full mode** (all images): ~3-5 hours (with GPU)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8a5CMr0HNI1",
        "outputId": "2d855dbd-b9c1-4dac-adf6-4d3965662a9e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë  üéØ CELLULE DE CONFIGURATION STANDALONE - COPIER-COLLER DANS VOS NOTEBOOKS ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "INSTRUCTIONS:\n",
        "-------------\n",
        "1. Copiez TOUT le contenu de cette cellule\n",
        "2. Collez-le comme PREMI√àRE CELLULE de votre notebook\n",
        "3. Ex√©cutez la cellule\n",
        "4. La configuration est pr√™te √† l'emploi !\n",
        "\n",
        "Cette cellule est 100% autonome et fonctionne partout :\n",
        "‚úÖ Google Colab (clone + installe automatiquement)\n",
        "‚úÖ WSL / Linux Local\n",
        "‚úÖ Tout environnement Jupyter\n",
        "\n",
        "APR√àS EX√âCUTION, UTILISEZ L'OBJET 'config':\n",
        "--------------------------------------------\n",
        "‚ñ∂ config.data_dir              # Chemin du dataset\n",
        "‚ñ∂ config.models_dir            # R√©pertoire des mod√®les\n",
        "‚ñ∂ config.results_dir           # R√©pertoire des r√©sultats\n",
        "‚ñ∂ config.classes               # Liste des classes\n",
        "‚ñ∂ config.img_size              # Tuple (width, height)\n",
        "‚ñ∂ config.img_channels          # Nombre de canaux (1=grayscale, 3=RGB)\n",
        "‚ñ∂ config.batch_size            # Taille des batchs\n",
        "‚ñ∂ config.epochs                # Nombre d'√©poques\n",
        "‚ñ∂ config.learning_rate         # Learning rate\n",
        "‚ñ∂ config.validation_split      # Proportion pour validation\n",
        "‚ñ∂ config.gradcam_alpha         # Alpha pour Grad-CAM\n",
        "‚ñ∂ config.shap_max_evals        # Evaluations SHAP\n",
        "‚ñ∂ config.confidence_high_threshold  # Seuil confiance haute\n",
        "... et bien plus !\n",
        "\n",
        "VARIABLES GLOBALES:\n",
        "-------------------\n",
        "‚Ä¢ config: Objet Config complet (tous les param√®tres du projet)\n",
        "‚Ä¢ ENV: Environnement d√©tect√© ('colab', 'wsl', 'local')\n",
        "‚Ä¢ Tous les transformers import√©s et pr√™ts √† l'emploi\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS STANDARDS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# D√âTECTION AUTOMATIQUE DE L'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def detect_environment():\n",
        "    \"\"\"D√©tecte l'environnement (colab, wsl, local)\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except ImportError:\n",
        "        is_wsl = os.path.exists('/proc/version') and 'microsoft' in open('/proc/version').read().lower()\n",
        "        return \"wsl\" if is_wsl else \"local\"\n",
        "\n",
        "ENV = detect_environment()\n",
        "print(f\"üåç Environnement: {ENV.upper()}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# BOOTSTRAP COLAB (Clone + Install si n√©cessaire)\n",
        "# =============================================================================\n",
        "\n",
        "if ENV == \"colab\":\n",
        "    print(\"\\nüöÄ Bootstrap Colab...\")\n",
        "\n",
        "    os.chdir('/content')\n",
        "    if not os.path.exists('/content/Data_Pipeline'):\n",
        "        print(\"üì• Clonage du repository...\")\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/L-Poca/Data_Pipeline.git'], check=True)\n",
        "\n",
        "    os.chdir('/content/Data_Pipeline')\n",
        "\n",
        "    # Checkout de la branche rafael_cleaning\n",
        "    result = subprocess.run(\n",
        "        ['git', 'checkout', '-b', 'rafael_cleaning', 'origin/rafael_cleaning'],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        # Si la branche locale existe d√©j√†, juste switcher\n",
        "        subprocess.run(['git', 'checkout', 'rafael_cleaning'], capture_output=True)\n",
        "\n",
        "    # Installation du package en mode √©ditable (sans d√©pendances - d√©tection Colab dans setup.py)\n",
        "    print(\"üì¶ Installation du package...\")\n",
        "    result = subprocess.run(['pip', 'install', '-e', '.', '--quiet'], capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"‚ö†Ô∏è Erreur installation: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Package install√©\")\n",
        "\n",
        "    print(\"üíæ Montage Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Extraction dataset\n",
        "    archive_data = '/content/drive/MyDrive/DS_COVID/archive_covid.zip'\n",
        "    if os.path.exists(archive_data):\n",
        "        print(\"üì¶ Extraction dataset...\")\n",
        "        os.makedirs('./data/raw/', exist_ok=True)\n",
        "        subprocess.run(['unzip', '-o', '-q', archive_data, '-d', './data/raw/COVID-19_Radiography_Dataset/'])\n",
        "\n",
        "    # Extraction models\n",
        "    archive_models = '/content/drive/MyDrive/DS_COVID/inceptionv3_best.zip'\n",
        "    if os.path.exists(archive_models):\n",
        "        print(\"üì¶ Extraction models...\")\n",
        "        os.makedirs('./models/', exist_ok=True)\n",
        "        subprocess.run(['unzip', '-o', '-q', archive_models, '-d', './models/'])\n",
        "\n",
        "    print(\"‚úÖ Bootstrap termin√©\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION DES CHEMINS\n",
        "# =============================================================================\n",
        "\n",
        "# D√©terminer project_root selon l'environnement\n",
        "if ENV == \"colab\":\n",
        "    project_root = Path('/content/Data_Pipeline')\n",
        "elif ENV == \"wsl\":\n",
        "    project_root = Path('/home/lena/Data_Pipeline')\n",
        "    #project_root = Path.cwd().parent.parent\n",
        "else:  # local\n",
        "    # Depuis un notebook dans src/notebooks/\n",
        "    project_root = Path.cwd().parent.parent\n",
        "\n",
        "# V√©rification du mod√®le en local (WSL ou autre)\n",
        "if ENV != \"colab\":\n",
        "    models_dir = project_root / 'models'\n",
        "    model_path = models_dir / 'inceptionv3_best.keras'\n",
        "\n",
        "    if model_path.exists():\n",
        "        print(f\"‚úÖ Mod√®le InceptionV3 trouv√©: {model_path}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Mod√®le InceptionV3 non trouv√©: {model_path}\")\n",
        "        print(f\"   Veuillez placer inceptionv3_best.keras dans {models_dir}/\")\n",
        "\n",
        "# Ajouter src/ au sys.path pour les imports\n",
        "# src_path = str(project_root / 'src')\n",
        "# if src_path not in sys.path:\n",
        "#     sys.path.insert(0, src_path)\n",
        "#     print(f\"‚úÖ Chemin src/ ajout√©: {src_path}\")\n",
        "\n",
        "# Charger la configuration depuis JSON\n",
        "from src.utils.config import build_config\n",
        "\n",
        "config = build_config(project_root, ENV)\n",
        "\n",
        "print(f\"\\nüéØ Configuration charg√©e depuis config/{ENV}_config.json\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS DES TRANSFORMERS\n",
        "# =============================================================================\n",
        "\n",
        "try:\n",
        "    from src.features.Pipelines.Transformateurs.image_loaders import ImageLoader\n",
        "    from src.features.Pipelines.Transformateurs.image_preprocessing import (\n",
        "        ImageResizer, ImageNormalizer, ImageFlattener, ImageMasker\n",
        "    )\n",
        "    from src.features.Pipelines.Transformateurs.image_augmentation import (\n",
        "        ImageAugmenter, ImageRandomCropper\n",
        "    )\n",
        "    from src.features.Pipelines.Transformateurs.image_features import (\n",
        "        ImageHistogram, ImagePCA, ImageStandardScaler\n",
        "    )\n",
        "    print(\"‚úÖ Transformers import√©s\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur import transformers: {e}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS ML/DL\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION MATPLOTLIB (utilise config pour les param√®tres)\n",
        "# =============================================================================\n",
        "\n",
        "plt.rcParams['figure.figsize'] = config.figure_size\n",
        "plt.rcParams['figure.dpi'] = config.dpi\n",
        "plt.style.use(config.plot_style)\n",
        "sns.set_palette(config.color_palette)\n",
        "\n",
        "# =============================================================================\n",
        "# AFFICHAGE DU R√âSUM√â\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ CONFIGURATION PR√äTE - Data Pipeline\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìÇ Projet:       {config.project_root}\")\n",
        "print(f\"üìä Dataset:      {config.data_dir}\")\n",
        "print(f\"üíæ Mod√®les:      {config.models_dir}\")\n",
        "print(f\"üìà R√©sultats:    {config.results_dir}\")\n",
        "print(f\"üìê Dataset:      {'‚úÖ Accessible' if config.data_dir.exists() else '‚ùå Introuvable'}\")\n",
        "print()\n",
        "print(f\"üè∑Ô∏è  Classes:     {', '.join(config.classes)} ({config.num_classes} classes)\")\n",
        "print(f\"üéõÔ∏è  Images:      {config.img_size} | {config.img_channels} canaux\")\n",
        "print(f\"üîß Training:     Batch={config.batch_size} | Epochs={config.epochs} | LR={config.learning_rate}\")\n",
        "print(f\"ÔøΩ Splits:       Train/Val={1-config.validation_split:.0%} | Val={config.validation_split:.0%} | Test={config.test_split:.0%}\")\n",
        "print()\n",
        "print(f\"üé® Viz:          Style={config.plot_style} | Palette={config.color_palette}\")\n",
        "print(f\"üìè Figures:      {config.figure_size} @ {config.dpi} DPI\")\n",
        "print()\n",
        "print(f\"üîç Interpr√©t.:   GradCAM Œ±={config.gradcam_alpha} | SHAP evals={config.shap_max_evals}\")\n",
        "print(f\"üìâ Seuils conf.: High={config.confidence_high_threshold} | Medium={config.confidence_medium_threshold}\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüí° Variable principale:\")\n",
        "print(\"   ‚Ä¢ config: Objet Config complet (acc√®s √† TOUS les param√®tres)\")\n",
        "print(\"   ‚Ä¢ ENV: Environnement actuel\")\n",
        "print()\n",
        "print(\"üìö Exemples d'utilisation:\")\n",
        "print(\"   config.data_dir          # Chemin du dataset\")\n",
        "print(\"   config.classes           # Liste des classes\")\n",
        "print(\"   config.img_size          # Tuple (width, height)\")\n",
        "print(\"   config.batch_size        # Taille des batchs\")\n",
        "print(\"   config.models_dir        # R√©pertoire des mod√®les\")\n",
        "print(\"   config.gradcam_alpha     # Param√®tres d'interpr√©tabilit√©\")\n",
        "print()\n",
        "print(\"üéØ Transformers disponibles:\")\n",
        "print(\"   ‚Ä¢ ImageLoader, ImageResizer, ImageNormalizer, ImageFlattener, ImageMasker\")\n",
        "print(\"   ‚Ä¢ ImageAugmenter, ImageRandomCropper\")\n",
        "print(\"   ‚Ä¢ ImageHistogram, ImagePCA, ImageStandardScaler\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZCLGOQCHNI2"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50 if ENV == \"colab\" else 100\n",
        "config.batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1z72TuHNI2"
      },
      "source": [
        "## üìö Section 1: Imports ML/DL Compl√©mentaires\n",
        "\n",
        "Import des biblioth√®ques additionnelles pour le machine learning classique et le deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-ina0O_HNI2",
        "outputId": "046bc254-92df-4d7e-db83-4ed8b101457d"
      },
      "outputs": [],
      "source": [
        "if ENV == 'colab' :\n",
        "  !pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xreZ0xFHNI2",
        "outputId": "45cb4a18-4de6-474c-b8ae-07a202589831"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS ML/DL COMPL√âMENTAIRES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPORTS ML/DL COMPL√âMENTAIRES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    f1_score, precision_score, recall_score, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, cohen_kappa_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        ")\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        ")\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import (\n",
        "    InceptionV3, ResNet50, ResNet152, VGG16, VGG19,\n",
        "    EfficientNetB0, EfficientNetB3, EfficientNetB7,\n",
        "    DenseNet121, DenseNet169, MobileNetV2, Xception\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"‚úÖ Optuna disponible\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Optuna non disponible. Installez-le pour le tuning avanc√© : pip install optuna\")\n",
        "\n",
        "# Interpretability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "    print(\"‚úÖ SHAP disponible\")\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è SHAP non disponible. Installez-le : pip install shap\")\n",
        "\n",
        "try:\n",
        "    import lime\n",
        "    from lime import lime_image\n",
        "    LIME_AVAILABLE = True\n",
        "    print(\"‚úÖ LIME disponible\")\n",
        "except ImportError:\n",
        "    LIME_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è LIME non disponible. Installez-le : pip install lime\")\n",
        "\n",
        "# Class imbalance\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    from imblearn.combine import SMOTETomek\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "    print(\"‚úÖ imbalanced-learn disponible\")\n",
        "except ImportError:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è imbalanced-learn non disponible. Installez-le : pip install imbalanced-learn\")\n",
        "\n",
        "# Utils\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json as json_lib\n",
        "import warnings\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n‚úÖ Imports ML/DL complets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rndejq_uHNI3"
      },
      "source": [
        "## üìä Section 2: Chargement des Donn√©es\n",
        "\n",
        "Configuration du dataset et chargement des images avec preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Notebook_begin_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "0tl0rg9aHNI3",
        "outputId": "30cde721-43ad-4d1d-d549-1c5b9016c6f6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION DU DATASET\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONFIGURATION DU DATASET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ‚ö†Ô∏è PARAM√àTRE IMPORTANT : Nombre d'images par classe\n",
        "# None = toutes les images (~21K total)\n",
        "# 100/500/1000 = tests rapides\n",
        "N_IMAGES_PER_CLASS = 2000 #None # Modifier pour tests rapides\n",
        "\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "print(f\"   Images par classe: {N_IMAGES_PER_CLASS if N_IMAGES_PER_CLASS else 'TOUTES'}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CHARGEMENT DES DONN√âES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CHARGEMENT DES DONN√âES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.notebooks import load_dataset, create_preprocessing_pipeline\n",
        "\n",
        "# Charger les chemins des images\n",
        "image_paths, mask_paths, labels, labels_int = load_dataset(\n",
        "    data_dir=config.data_dir,\n",
        "    categories=config.classes,\n",
        "    n_images_per_class=N_IMAGES_PER_CLASS,\n",
        "    load_masks=False,  # Pas besoin des masques pour classification\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset charg√©:\")\n",
        "print(f\"   Total images: {len(image_paths)}\")\n",
        "print(f\"   Classes: {config.classes}\")\n",
        "print(f\"   Distribution: {np.bincount(labels_int)}\")\n",
        "\n",
        "# Cr√©er pipeline de preprocessing\n",
        "pipeline_img = create_preprocessing_pipeline(\n",
        "    img_size=config.img_size,\n",
        "    color_mode='L',  # Grayscale\n",
        "    mask_paths=None,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Charger et preprocesser\n",
        "print(\"\\nüìä Preprocessing des images...\")\n",
        "images = pipeline_img.fit_transform(image_paths)\n",
        "images = images.astype('float32') / 255.0  # Normaliser [0, 1]\n",
        "\n",
        "print(f\"\\n‚úÖ Images pr√©par√©es:\")\n",
        "print(f\"   Shape: {images.shape}\")\n",
        "print(f\"   Range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "\n",
        "# Visualisation √©chantillons\n",
        "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
        "for i in range(4):\n",
        "    for j in range(8):\n",
        "        idx = i * (len(images) // 4) + j\n",
        "        if idx < len(images):\n",
        "            axes[i, j].imshow(images[idx], cmap='gray')\n",
        "            if j == 0:\n",
        "                axes[i, j].set_ylabel(config.classes[i], rotation=0, ha='right', va='center')\n",
        "            axes[i, j].axis('off')\n",
        "plt.suptitle('√âchantillons du Dataset', size=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1o00JQFHNI3"
      },
      "source": [
        "## ‚öñÔ∏è Section 3: Analyse du Class Imbalance\n",
        "\n",
        "Analyse de la distribution des classes et identification des d√©s√©quilibres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zKdPe38HNI3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ANALYSE DU CLASS IMBALANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ANALYSE DU CLASS IMBALANCE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Distribution\n",
        "unique, counts = np.unique(labels_int, return_counts=True)\n",
        "total = len(labels_int)\n",
        "\n",
        "print(\"\\nüìä Distribution actuelle:\")\n",
        "for cls_idx, count in zip(unique, counts):\n",
        "    percentage = (count / total) * 100\n",
        "    print(f\"   {config.classes[cls_idx]:20s}: {count:6d} images ({percentage:5.2f}%)\")\n",
        "\n",
        "# Ratio de d√©s√©quilibre\n",
        "max_count, min_count = counts.max(), counts.min()\n",
        "imbalance_ratio = max_count / min_count\n",
        "print(f\"\\n‚ö†Ô∏è Ratio de d√©s√©quilibre: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"   ‚Üí Class imbalance significatif d√©tect√©!\")\n",
        "    print(\"   ‚Üí Strat√©gies de r√©√©quilibrage n√©cessaires\")\n",
        "\n",
        "# Visualisation\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors = sns.color_palette('husl', len(config.classes))\n",
        "bars = ax.bar(config.classes, counts, color=colors)\n",
        "ax.set_ylabel('Nombre d\\'images', fontsize=11)\n",
        "ax.set_title('Distribution des Classes (D√©s√©quilibr√©e)', fontsize=13, weight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Ajouter pourcentages sur les barres\n",
        "for bar, count in zip(bars, counts):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{count}\\n({count/total*100:.1f}%)',\n",
        "            ha='center', va='bottom', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# STRAT√âGIES DE R√â√âQUILIBRAGE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STRAT√âGIES DE R√â√âQUILIBRAGE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "imbalance_strategies = {}\n",
        "\n",
        "# 1. CLASS WEIGHTS (sklearn)\n",
        "print(\"\\n1Ô∏è‚É£ Class Weights (sklearn)\")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights_array = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(labels_int),\n",
        "    y=labels_int\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights_array))\n",
        "\n",
        "print(\"   Poids calcul√©s:\")\n",
        "for cls_idx, weight in class_weights_dict.items():\n",
        "    print(f\"      {config.classes[cls_idx]:20s}: {weight:.3f}\")\n",
        "\n",
        "imbalance_strategies['class_weights'] = class_weights_dict\n",
        "\n",
        "# 2. SMOTE (si disponible)\n",
        "if IMBLEARN_AVAILABLE:\n",
        "    print(\"\\n2Ô∏è‚É£ SMOTE (Synthetic Minority Over-sampling)\")\n",
        "    print(\"   ‚úÖ Disponible (sera appliqu√© lors du training ML)\")\n",
        "    imbalance_strategies['smote_available'] = True\n",
        "else:\n",
        "    print(\"\\n2Ô∏è‚É£ SMOTE non disponible\")\n",
        "    imbalance_strategies['smote_available'] = False\n",
        "\n",
        "# 3. RANDOM OVERSAMPLING\n",
        "print(\"\\n3Ô∏è‚É£ Random Oversampling\")\n",
        "print(\"   ‚úÖ Disponible (duplication d'images de la classe minoritaire)\")\n",
        "imbalance_strategies['oversampling'] = True\n",
        "\n",
        "# 4. RANDOM UNDERSAMPLING\n",
        "print(\"\\n4Ô∏è‚É£ Random Undersampling\")\n",
        "print(\"   ‚úÖ Disponible (r√©duction de la classe majoritaire)\")\n",
        "imbalance_strategies['undersampling'] = True\n",
        "\n",
        "print(\"\\n‚úÖ Strat√©gies identifi√©es et pr√™tes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVOC1NIfHNI3"
      },
      "source": [
        "## üîÄ Section 4: Train/Val/Test Split Stratifi√©\n",
        "\n",
        "S√©paration stratifi√©e en ensembles d'entra√Ænement, validation et test (70/15/15)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6fRnEoTHNI3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SPLIT TRAIN/VAL/TEST STRATIFI√â\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SPLIT TRAIN/VAL/TEST STRATIFI√â\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# √âtape 1: Split 70/30 (train / temp)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    images, labels_int,\n",
        "    test_size=0.30,\n",
        "    random_state=config.random_seed,\n",
        "    stratify=labels_int\n",
        ")\n",
        "\n",
        "# √âtape 2: Split 30 ‚Üí 15/15 (validation / test)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=config.random_seed,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Splits cr√©√©s:\")\n",
        "print(f\"   Train: {X_train.shape[0]:5d} images ({X_train.shape[0]/len(images)*100:.1f}%)\")\n",
        "print(f\"      Distribution: {np.bincount(y_train)}\")\n",
        "print(f\"   Val:   {X_val.shape[0]:5d} images ({X_val.shape[0]/len(images)*100:.1f}%)\")\n",
        "print(f\"      Distribution: {np.bincount(y_val)}\")\n",
        "print(f\"   Test:  {X_test.shape[0]:5d} images ({X_test.shape[0]/len(images)*100:.1f}%)\")\n",
        "print(f\"      Distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# V√©rifier stratification\n",
        "print(\"\\n‚úÖ V√©rification de la stratification:\")\n",
        "print(f\"   {'Classe':<20s} {'Train %':>10s} {'Val %':>10s} {'Test %':>10s}\")\n",
        "print(\"   \" + \"-\" * 50)\n",
        "for i, cls_name in enumerate(config.classes):\n",
        "    train_pct = (y_train == i).sum() / len(y_train) * 100\n",
        "    val_pct = (y_val == i).sum() / len(y_val) * 100\n",
        "    test_pct = (y_test == i).sum() / len(y_test) * 100\n",
        "    print(f\"   {cls_name:<20s} {train_pct:>9.2f}% {val_pct:>9.2f}% {test_pct:>9.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDQ4oViIHNI4"
      },
      "source": [
        "## üîÑ Section 5: Data Augmentation\n",
        "\n",
        "Configuration de l'augmentation de donn√©es pour am√©liorer la g√©n√©ralisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk52Mtp8HNI4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA AUGMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA AUGMENTATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Configuration\n",
        "augmentation_config = {\n",
        "    'rotation_range': 15,\n",
        "    'width_shift_range': 0.1,\n",
        "    'height_shift_range': 0.1,\n",
        "    'horizontal_flip': True,\n",
        "    'zoom_range': 0.15,\n",
        "    'shear_range': 0.1,\n",
        "    'fill_mode': 'nearest'\n",
        "}\n",
        "\n",
        "print(\"\\nüîß Configuration:\")\n",
        "for key, value in augmentation_config.items():\n",
        "    print(f\"   {key:25s}: {value}\")\n",
        "\n",
        "# Cr√©er g√©n√©rateurs\n",
        "train_datagen = ImageDataGenerator(**augmentation_config)\n",
        "val_datagen = ImageDataGenerator()  # Pas d'augmentation pour validation\n",
        "\n",
        "print(\"\\n‚úÖ G√©n√©rateurs cr√©√©s\")\n",
        "\n",
        "# Visualisation de l'effet\n",
        "print(\"\\nüì∏ Visualisation de l'augmentation...\")\n",
        "sample_img = X_train[0:1]\n",
        "if sample_img.ndim == 3:\n",
        "    sample_img = sample_img[..., np.newaxis]\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "axes[0].imshow(sample_img[0, :, :, 0], cmap='gray')\n",
        "axes[0].set_title('Original', fontsize=10)\n",
        "axes[0].axis('off')\n",
        "\n",
        "aug_iter = train_datagen.flow(sample_img, batch_size=1)\n",
        "for i in range(1, 9):\n",
        "    aug_img = next(aug_iter)[0]\n",
        "    axes[i].imshow(aug_img[:, :, 0], cmap='gray')\n",
        "    axes[i].set_title(f'Augment√©e {i}', fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Effet de l\\'Augmentation de Donn√©es', size=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llGNBgaJHNI4"
      },
      "source": [
        "## ü§ñ Section 6: Baseline ML Models\n",
        "\n",
        "Entra√Ænement de 9 mod√®les ML classiques avec 3 types de features (PCA, Histogram, Combined)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWsibK1qHNI4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PR√âPARATION DES FEATURES POUR ML\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PR√âPARATION DES FEATURES POUR ML\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.features.Pipelines.Transformateurs.image_features import (\n",
        "    ImagePCA, ImageStandardScaler, ImageHistogram\n",
        ")\n",
        "from src.features.Pipelines.Transformateurs.image_preprocessing import ImageFlattener\n",
        "\n",
        "# 1. Pipeline PCA\n",
        "print(\"\\n1Ô∏è‚É£ Features PCA...\")\n",
        "n_pca_components = min(50, X_train.shape[0] - 1)\n",
        "\n",
        "pipeline_pca = Pipeline([\n",
        "    ('flatten', ImageFlattener(verbose=True)),\n",
        "    ('scale', ImageStandardScaler(verbose=True)),\n",
        "    ('pca', ImagePCA(n_components=n_pca_components, random_state=config.random_seed, verbose=True))\n",
        "])\n",
        "\n",
        "X_train_pca = pipeline_pca.fit_transform(X_train)\n",
        "X_val_pca = pipeline_pca.transform(X_val)\n",
        "X_test_pca = pipeline_pca.transform(X_test)\n",
        "\n",
        "pca_obj = pipeline_pca.named_steps['pca']\n",
        "print(f\"   ‚úÖ {X_train_pca.shape[1]} composantes\")\n",
        "print(f\"   Variance expliqu√©e: {pca_obj.explained_variance_ratio_.sum():.2%}\")\n",
        "\n",
        "# 2. Pipeline Histogram\n",
        "print(\"\\n2Ô∏è‚É£ Features Histogram...\")\n",
        "pipeline_hist = Pipeline([\n",
        "    ('histogram', ImageHistogram(bins=64, density=True, verbose=True)),\n",
        "    ('scale', ImageStandardScaler(verbose=True))\n",
        "])\n",
        "\n",
        "X_train_hist = pipeline_hist.fit_transform(X_train)\n",
        "X_val_hist = pipeline_hist.transform(X_val)\n",
        "X_test_hist = pipeline_hist.transform(X_test)\n",
        "\n",
        "print(f\"   ‚úÖ {X_train_hist.shape[1]} bins\")\n",
        "\n",
        "# 3. Features combin√©es\n",
        "print(\"\\n3Ô∏è‚É£ Features Combin√©es (PCA + Histogram)...\")\n",
        "X_train_combined = np.hstack([X_train_pca, X_train_hist])\n",
        "X_val_combined = np.hstack([X_val_pca, X_val_hist])\n",
        "X_test_combined = np.hstack([X_test_pca, X_test_hist])\n",
        "\n",
        "print(f\"   ‚úÖ {X_train_combined.shape[1]} features\")\n",
        "print(f\"      PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"      Histogram: {X_train_hist.shape[1]}\")\n",
        "\n",
        "# =============================================================================\n",
        "# BASELINE ML MODELS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BASELINE ML MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# D√©finir les mod√®les\n",
        "ml_models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        random_state=config.random_seed,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=config.random_seed\n",
        "    ),\n",
        "    'AdaBoost': AdaBoostClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.5,\n",
        "        random_state=config.random_seed\n",
        "    ),\n",
        "    'SVM (RBF)': SVC(\n",
        "        kernel='rbf',\n",
        "        C=1.0,\n",
        "        gamma='scale',\n",
        "        random_state=config.random_seed,\n",
        "        class_weight='balanced',\n",
        "        probability=True\n",
        "    ),\n",
        "    'SVM (Linear)': SVC(\n",
        "        kernel='linear',\n",
        "        C=1.0,\n",
        "        random_state=config.random_seed,\n",
        "        class_weight='balanced',\n",
        "        probability=True\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=config.random_seed,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'KNN': KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(\n",
        "        max_depth=10,\n",
        "        random_state=config.random_seed,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# Feature sets\n",
        "feature_sets = {\n",
        "    'PCA': (X_train_pca, X_val_pca, X_test_pca),\n",
        "    'Histogram': (X_train_hist, X_val_hist, X_test_hist),\n",
        "    'Combined': (X_train_combined, X_val_combined, X_test_combined)\n",
        "}\n",
        "\n",
        "# Stocker r√©sultats\n",
        "ml_results = {}\n",
        "\n",
        "print(f\"\\nüöÄ Entra√Ænement de {len(ml_models)} mod√®les √ó {len(feature_sets)} feature sets\")\n",
        "print(f\"   Total: {len(ml_models) * len(feature_sets)} combinaisons\\n\")\n",
        "\n",
        "for feat_name, (X_tr, X_va, X_te) in feature_sets.items():\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"FEATURES: {feat_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    for model_name, model in tqdm(ml_models.items(), desc=f\"Training {feat_name}\"):\n",
        "        key = f\"{model_name} ({feat_name})\"\n",
        "        #print(f\"\\nüîß Mod√®le: {key}\", end=\"/r\")\n",
        "        \n",
        "        # Training\n",
        "        start_time = time.time()\n",
        "        model.fit(X_tr, y_train)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_train = model.predict(X_tr)\n",
        "        y_pred_val = model.predict(X_va)\n",
        "        y_pred_test = model.predict(X_te)\n",
        "\n",
        "        # Inference time\n",
        "        start_time = time.time()\n",
        "        _ = model.predict(X_te)\n",
        "        inference_time = (time.time() - start_time) / len(X_te) * 1000\n",
        "\n",
        "        # Metrics\n",
        "        ml_results[key] = {\n",
        "            'model': model,\n",
        "            'feature_type': feat_name,\n",
        "            'train_acc': accuracy_score(y_train, y_pred_train),\n",
        "            'val_acc': accuracy_score(y_val, y_pred_val),\n",
        "            'test_acc': accuracy_score(y_test, y_pred_test),\n",
        "            'f1_weighted': f1_score(y_test, y_pred_test, average='weighted'),\n",
        "            'f1_macro': f1_score(y_test, y_pred_test, average='macro'),\n",
        "            'precision': precision_score(y_test, y_pred_test, average='weighted'),\n",
        "            'recall': recall_score(y_test, y_pred_test, average='weighted'),\n",
        "            'train_time': train_time,\n",
        "            'inference_time_ms': inference_time,\n",
        "            'y_pred_test': y_pred_test,\n",
        "            'y_pred_val': y_pred_val\n",
        "        }\n",
        "\n",
        "# Tableau r√©capitulatif\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"R√âSULTATS ML MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n{'Mod√®le':<40s} {'Train':>8s} {'Val':>8s} {'Test':>8s} {'F1':>8s}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for key in sorted(ml_results.keys(), key=lambda x: ml_results[x]['test_acc'], reverse=True):\n",
        "    res = ml_results[key]\n",
        "    print(f\"{key:<40s} {res['train_acc']:>8.4f} {res['val_acc']:>8.4f} {res['test_acc']:>8.4f} {res['f1_weighted']:>8.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Baseline ML termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBYXmdvJHNI4"
      },
      "source": [
        "## üß† Section 7: Custom CNN Architectures\n",
        "\n",
        "Construction et entra√Ænement de 3 architectures CNN personnalis√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia0Cq4PZHNI4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CUSTOM CNN ARCHITECTURES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CUSTOM CNN ARCHITECTURES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.notebooks import (\n",
        "    build_simple_cnn,\n",
        "    build_medium_cnn,\n",
        "    build_deep_cnn,\n",
        "    compile_model,\n",
        "    create_callbacks\n",
        ")\n",
        "\n",
        "# Prepare data for CNN\n",
        "X_train_cnn = X_train[..., np.newaxis]  # Add channel dimension\n",
        "X_val_cnn = X_val[..., np.newaxis]\n",
        "X_test_cnn = X_test[..., np.newaxis]\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_cat = to_categorical(y_train, num_classes=config.num_classes)\n",
        "y_val_cat = to_categorical(y_val, num_classes=config.num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=config.num_classes)\n",
        "\n",
        "# Define CNN architectures with their builder functions\n",
        "cnn_architectures = {\n",
        "    'CNN_Simple': {\n",
        "        'builder': build_simple_cnn,\n",
        "        'description': '2 conv blocks (32‚Üí64), 1 dense (128)'\n",
        "    },\n",
        "    'CNN_Medium': {\n",
        "        'builder': build_medium_cnn,\n",
        "        'description': '3 conv blocks (32‚Üí64‚Üí128), 2 dense (256‚Üí128)'\n",
        "    },\n",
        "    'CNN_Deep': {\n",
        "        'builder': build_deep_cnn,\n",
        "        'description': '4 conv blocks (32‚Üí64‚Üí128‚Üí256), 2 dense (512‚Üí256)'\n",
        "    }\n",
        "}\n",
        "\n",
        "cnn_results = {}\n",
        "\n",
        "for arch_name, arch_config in cnn_architectures.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training: {arch_name}\")\n",
        "    print(f\"Description: {arch_config['description']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Build model using the appropriate builder function\n",
        "    model = arch_config['builder'](\n",
        "        input_shape=(config.img_size[0], config.img_size[1], 1),\n",
        "        num_classes=config.num_classes,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Compile\n",
        "    model = compile_model(model, learning_rate=config.learning_rate, verbose=True)\n",
        "\n",
        "    print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
        "\n",
        "    # Callbacks\n",
        "    model_save_dir = config.models_dir / arch_name\n",
        "    callbacks = create_callbacks(\n",
        "        models_dir=model_save_dir,\n",
        "        patience_early_stop=10,\n",
        "        patience_reduce_lr=5,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        X_train_cnn, y_train_cat,\n",
        "        validation_data=(X_val_cnn, y_val_cat),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=config.batch_size,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weights_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate - model.evaluate() returns [loss, accuracy, auc, precision, recall]\n",
        "    eval_results = model.evaluate(X_test_cnn, y_test_cat, verbose=1)\n",
        "    test_loss = eval_results[0]\n",
        "    test_acc = eval_results[1]\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_prob = model.predict(X_test_cnn)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    # Store results\n",
        "    cnn_results[arch_name] = {\n",
        "        'model': model,\n",
        "        'history': history.history,\n",
        "        'test_acc': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
        "        'train_time': train_time,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "    print(f\"\\n‚úÖ {arch_name}: Test Acc = {test_acc:.4f}, F1 = {cnn_results[arch_name]['f1_weighted']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Custom CNN training termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE6hJuapHNI4"
      },
      "source": [
        "## üîÑ Section 8: Transfer Learning\n",
        "\n",
        "Entra√Ænement de 12 architectures pr√©-entra√Æn√©es avec fine-tuning en 2 phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXIw1716HNI4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRANSFER LEARNING - 4 MODELS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRANSFER LEARNING - 4 ARCHITECTURES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.notebooks import build_transfer_learning_model, unfreeze_top_layers\n",
        "\n",
        "# Define architectures (use strings, not classes)\n",
        "transfer_architectures = {\n",
        "    'InceptionV3': 'InceptionV3',\n",
        "    'ResNet50': 'ResNet50',\n",
        "    'VGG16': 'VGG16',\n",
        "    'EfficientNetB0': 'EfficientNetB0'\n",
        "}\n",
        "\n",
        "# Prepare RGB data (most models expect 3 channels)\n",
        "X_train_rgb = np.repeat(X_train[..., np.newaxis], 3, axis=-1)\n",
        "X_val_rgb = np.repeat(X_val[..., np.newaxis], 3, axis=-1)\n",
        "X_test_rgb = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
        "\n",
        "transfer_results = {}\n",
        "\n",
        "for arch_name in transfer_architectures.keys():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Transfer Learning: {arch_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    try:\n",
        "        # Phase 1: Feature extraction\n",
        "        print(f\"\\nPhase 1: Feature extraction (frozen base)\")\n",
        "\n",
        "        model, base_model = build_transfer_learning_model(\n",
        "            base_model_name=arch_name,\n",
        "            input_shape=(config.img_size[0], config.img_size[1], 3),\n",
        "            num_classes=config.num_classes,\n",
        "            freeze_base=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        model = compile_model(model, learning_rate=config.learning_rate, verbose=True)\n",
        "\n",
        "        model_save_dir_p1 = config.models_dir / f\"{arch_name}_phase1\"\n",
        "        callbacks_phase1 = create_callbacks(\n",
        "            models_dir=model_save_dir_p1,\n",
        "            patience_early_stop=5,\n",
        "            patience_reduce_lr=3,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        history_phase1 = model.fit(\n",
        "            X_train_rgb, y_train_cat,\n",
        "            validation_data=(X_val_rgb, y_val_cat),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=config.batch_size,\n",
        "            callbacks=callbacks_phase1,\n",
        "            class_weight=class_weights_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Phase 2: Fine-tuning\n",
        "        print(f\"Phase 2: Fine-tuning (unfrozen top layers)\")\n",
        "\n",
        "        model = unfreeze_top_layers(\n",
        "            base_model=base_model,\n",
        "            model=model,\n",
        "            n_layers=20,\n",
        "            learning_rate=config.learning_rate / 10,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        model_save_dir_p2 = config.models_dir / f\"{arch_name}_phase2\"\n",
        "        callbacks_phase2 = create_callbacks(\n",
        "            models_dir=model_save_dir_p2,\n",
        "            patience_early_stop=5,\n",
        "            patience_reduce_lr=3,\n",
        "            monitor='val_accuracy',\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        history_phase2 = model.fit(\n",
        "            X_train_rgb, y_train_cat,\n",
        "            validation_data=(X_val_rgb, y_val_cat),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=config.batch_size,\n",
        "            callbacks=callbacks_phase2,\n",
        "            class_weight=class_weights_dict,\n",
        "            verbose=1 \n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate - model.evaluate() returns [loss, accuracy, auc, precision, recall]\n",
        "        eval_results = model.evaluate(X_test_rgb, y_test_cat, verbose=1)\n",
        "        test_loss = eval_results[0]\n",
        "        test_acc = eval_results[1]\n",
        "\n",
        "        y_pred_prob = model.predict(X_test_rgb)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "        transfer_results[arch_name] = {\n",
        "            'model': model,\n",
        "            'history_phase1': history_phase1.history,\n",
        "            'history_phase2': history_phase2.history,\n",
        "            'test_acc': test_acc,\n",
        "            'test_loss': test_loss,\n",
        "            'f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
        "            'train_time': train_time,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        print(f\"‚úÖ {arch_name}: Test Acc = {test_acc:.4f}, F1 = {transfer_results[arch_name]['f1_weighted']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error with {arch_name}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n‚úÖ Transfer Learning termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGUGXMMEHNI4"
      },
      "source": [
        "## üéØ Section 9: Ensemble Methods\n",
        "\n",
        "M√©thodes d'ensemble pour am√©liorer les performances : Voting et Stacking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd2DOP4qHNI4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENSEMBLE METHODS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ENSEMBLE METHODS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Select top 3 ML models\n",
        "top_ml_models = sorted(\n",
        "    [(k, v) for k, v in ml_results.items()],\n",
        "    key=lambda x: x[1]['test_acc'],\n",
        "    reverse=True\n",
        ")[:3]\n",
        "\n",
        "print(\"\\nüèÜ Top 3 ML models pour ensemble:\")\n",
        "for model_name, results in top_ml_models:\n",
        "    print(f\"   {model_name}: {results['test_acc']:.4f}\")\n",
        "\n",
        "# Voting Classifier\n",
        "print(\"\\n1Ô∏è‚É£ Voting Classifier\")\n",
        "voting_estimators = [(name, res['model']) for name, res in top_ml_models]\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=voting_estimators,\n",
        "    voting='soft',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Use Combined features for ensemble\n",
        "voting_clf.fit(X_train_combined, y_train)\n",
        "y_pred_voting = voting_clf.predict(X_test_combined)\n",
        "voting_acc = accuracy_score(y_test, y_pred_voting)\n",
        "voting_f1 = f1_score(y_test, y_pred_voting, average='weighted')\n",
        "\n",
        "print(f\"   Voting Accuracy: {voting_acc:.4f}\")\n",
        "print(f\"   Voting F1: {voting_f1:.4f}\")\n",
        "\n",
        "# Stacking Classifier\n",
        "print(\"\\n2Ô∏è‚É£ Stacking Classifier\")\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=voting_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacking_clf.fit(X_train_combined, y_train)\n",
        "y_pred_stacking = stacking_clf.predict(X_test_combined)\n",
        "stacking_acc = accuracy_score(y_test, y_pred_stacking)\n",
        "stacking_f1 = f1_score(y_test, y_pred_stacking, average='weighted')\n",
        "\n",
        "print(f\"   Stacking Accuracy: {stacking_acc:.4f}\")\n",
        "print(f\"   Stacking F1: {stacking_f1:.4f}\")\n",
        "\n",
        "ensemble_results = {\n",
        "    'Voting': {'acc': voting_acc, 'f1': voting_f1, 'model': voting_clf},\n",
        "    'Stacking': {'acc': stacking_acc, 'f1': stacking_f1, 'model': stacking_clf}\n",
        "}\n",
        "\n",
        "print(\"\\n‚úÖ Ensemble methods termin√©s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUTPAGkpHNI4"
      },
      "source": [
        "## ‚öôÔ∏è Section 10: Hyperparameter Optimization\n",
        "\n",
        "Optimisation des hyperparam√®tres avec GridSearchCV et Optuna (optionnel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIMdQbbgHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYPERPARAMETER OPTIMIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# GridSearchCV pour Random Forest\n",
        "print(\"\\n1Ô∏è‚É£ GridSearchCV - Random Forest\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=config.random_seed, n_jobs=-1),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_grid.fit(X_train_combined, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
        "print(f\"Best CV score: {rf_grid.best_score_:.4f}\")\n",
        "\n",
        "# Test best model\n",
        "y_pred_grid = rf_grid.predict(X_test_combined)\n",
        "grid_acc = accuracy_score(y_test, y_pred_grid)\n",
        "print(f\"Test accuracy: {grid_acc:.4f}\")\n",
        "\n",
        "# Optuna optimization (if available)\n",
        "if OPTUNA_AVAILABLE:\n",
        "    print(\"\\n2Ô∏è‚É£ Optuna - Advanced Hyperparameter Tuning\")\n",
        "    print(\"   ‚úÖ Optuna disponible - lancer l'optimisation si n√©cessaire\")\n",
        "    print(\"   (Skipped for notebook efficiency)\")\n",
        "else:\n",
        "    print(\"\\n2Ô∏è‚É£ Optuna non disponible\")\n",
        "\n",
        "print(\"\\n‚úÖ Hyperparameter optimization termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDVkZyLAHNI5"
      },
      "source": [
        "## üîÅ Section 11: K-Fold Cross-Validation\n",
        "\n",
        "Validation crois√©e stratifi√©e √† 5 plis pour √©valuer la robustesse des mod√®les."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnf-s2WhHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# K-FOLD CROSS-VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"K-FOLD CROSS-VALIDATION (5-FOLD)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Select top 5 ML models\n",
        "top_models_for_cv = sorted(\n",
        "    [(k, v) for k, v in ml_results.items()],\n",
        "    key=lambda x: x[1]['test_acc'],\n",
        "    reverse=True\n",
        ")[:5]\n",
        "\n",
        "cv_results = {}\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=config.random_seed)\n",
        "\n",
        "print(\"\\nRunning 5-fold cross-validation...\\n\")\n",
        "\n",
        "for model_name, model_info in top_models_for_cv:\n",
        "    print(f\"Evaluating: {model_name}\")\n",
        "\n",
        "    # Use combined features\n",
        "    scores = cross_val_score(\n",
        "        model_info['model'],\n",
        "        X_train_combined,\n",
        "        y_train,\n",
        "        cv=skf,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    cv_results[model_name] = {\n",
        "        'mean': scores.mean(),\n",
        "        'std': scores.std(),\n",
        "        'scores': scores\n",
        "    }\n",
        "\n",
        "    print(f\"   Mean CV Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "\n",
        "# Visualization\n",
        "print(\"\\nüìä Visualisation des r√©sultats CV\")\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "model_names = list(cv_results.keys())\n",
        "means = [cv_results[m]['mean'] for m in model_names]\n",
        "stds = [cv_results[m]['std'] for m in model_names]\n",
        "\n",
        "x_pos = np.arange(len(model_names))\n",
        "ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels([m.split(' (')[0] for m in model_names], rotation=45, ha='right')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('5-Fold Cross-Validation Results', fontsize=13, weight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ K-Fold Cross-Validation termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFAMl0CZHNI5"
      },
      "source": [
        "## üìà Section 12: Learning Curves\n",
        "\n",
        "Analyse des courbes d'apprentissage pour d√©tecter l'overfitting/underfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dIsMZAGHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LEARNING CURVES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"LEARNING CURVES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.notebooks import plot_training_curves\n",
        "\n",
        "# Plot learning curves for CNN models\n",
        "print(\"\\nüìä Learning curves pour les CNNs personnalis√©s\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (arch_name, results) in enumerate(cnn_results.items()):\n",
        "    history = results['history']\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    ax.plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    ax.set_title(f'{arch_name}', fontsize=12, weight='bold')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot loss curves\n",
        "print(\"\\nüìä Loss curves pour les CNNs personnalis√©s\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (arch_name, results) in enumerate(cnn_results.items()):\n",
        "    history = results['history']\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.plot(history['loss'], label='Train Loss', linewidth=2)\n",
        "    ax.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    ax.set_title(f'{arch_name}', fontsize=12, weight='bold')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Learning curves g√©n√©r√©es\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYvXE3GrHNI5"
      },
      "source": [
        "## üìä Section 13: Model Evaluation\n",
        "\n",
        "√âvaluation d√©taill√©e des mod√®les avec m√©triques par classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjCGcskjHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get best ML model\n",
        "best_ml_key = max(ml_results.keys(), key=lambda k: ml_results[k]['test_acc'])\n",
        "best_ml = ml_results[best_ml_key]\n",
        "\n",
        "print(f\"\\nüèÜ Best ML Model: {best_ml_key}\")\n",
        "print(f\"   Test Accuracy: {best_ml['test_acc']:.4f}\")\n",
        "print(f\"   F1 Score (weighted): {best_ml['f1_weighted']:.4f}\")\n",
        "\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    best_ml['y_pred_test'],\n",
        "    target_names=config.classes,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Get best CNN model\n",
        "best_cnn_key = max(cnn_results.keys(), key=lambda k: cnn_results[k]['test_acc'])\n",
        "best_cnn = cnn_results[best_cnn_key]\n",
        "\n",
        "print(f\"\\nüèÜ Best CNN Model: {best_cnn_key}\")\n",
        "print(f\"   Test Accuracy: {best_cnn['test_acc']:.4f}\")\n",
        "print(f\"   F1 Score (weighted): {best_cnn['f1_weighted']:.4f}\")\n",
        "\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    best_cnn['y_pred'],\n",
        "    target_names=config.classes,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Get best Transfer Learning model\n",
        "if transfer_results:\n",
        "    best_tl_key = max(transfer_results.keys(), key=lambda k: transfer_results[k]['test_acc'])\n",
        "    best_tl = transfer_results[best_tl_key]\n",
        "\n",
        "    print(f\"\\nüèÜ Best Transfer Learning Model: {best_tl_key}\")\n",
        "    print(f\"   Test Accuracy: {best_tl['test_acc']:.4f}\")\n",
        "    print(f\"   F1 Score (weighted): {best_tl['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_test,\n",
        "        best_tl['y_pred'],\n",
        "        target_names=config.classes,\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "print(\"\\n‚úÖ Model evaluation termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlPUQnyPHNI5"
      },
      "source": [
        "## üî¢ Section 14: Confusion Matrices\n",
        "\n",
        "Matrices de confusion pour visualiser les erreurs de classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS-We-JyHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFUSION MATRICES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.notebooks import plot_confusion_matrix\n",
        "\n",
        "# Plot confusion matrices for top models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Best ML\n",
        "cm_ml = confusion_matrix(y_test, best_ml['y_pred_test'])\n",
        "ax = axes[0]\n",
        "sns.heatmap(cm_ml, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "            xticklabels=config.classes, yticklabels=config.classes)\n",
        "ax.set_title(f'ML: {best_ml_key}', fontsize=11, weight='bold')\n",
        "ax.set_ylabel('True Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "\n",
        "# Best CNN\n",
        "cm_cnn = confusion_matrix(y_test, best_cnn['y_pred'])\n",
        "ax = axes[1]\n",
        "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Greens', ax=ax,\n",
        "            xticklabels=config.classes, yticklabels=config.classes)\n",
        "ax.set_title(f'CNN: {best_cnn_key}', fontsize=11, weight='bold')\n",
        "ax.set_ylabel('True Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "\n",
        "# Best Transfer Learning\n",
        "if transfer_results:\n",
        "    cm_tl = confusion_matrix(y_test, best_tl['y_pred'])\n",
        "    ax = axes[2]\n",
        "    sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Oranges', ax=ax,\n",
        "                xticklabels=config.classes, yticklabels=config.classes)\n",
        "    ax.set_title(f'TL: {best_tl_key}', fontsize=11, weight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Confusion matrices g√©n√©r√©es\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpgtGR6YHNI5"
      },
      "source": [
        "## üîç Section 15: Feature Importance\n",
        "\n",
        "Analyse de l'importance des features pour les mod√®les ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gJkFJjCHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FEATURE IMPORTANCE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Feature importance for Random Forest\n",
        "if 'Random Forest' in best_ml_key:\n",
        "    print(\"\\nüìä Feature Importance - Random Forest\")\n",
        "\n",
        "    importances = best_ml['model'].feature_importances_\n",
        "    indices = np.argsort(importances)[::-1][:20]  # Top 20\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.barh(range(len(indices)), importances[indices], color='steelblue')\n",
        "    ax.set_yticks(range(len(indices)))\n",
        "    ax.set_yticklabels([f'Feature {i}' for i in indices])\n",
        "    ax.set_xlabel('Importance')\n",
        "    ax.set_title('Top 20 Feature Importances', fontsize=13, weight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Feature importance only available for tree-based models\")\n",
        "\n",
        "# CNN Filter Visualization\n",
        "print(\"\\nüìä CNN First Layer Filters Visualization\")\n",
        "\n",
        "if cnn_results:\n",
        "    first_cnn = list(cnn_results.values())[0]['model']\n",
        "\n",
        "    # Get first convolutional layer\n",
        "    for layer in first_cnn.layers:\n",
        "        if 'conv' in layer.name.lower():\n",
        "            filters, biases = layer.get_weights()\n",
        "            break\n",
        "\n",
        "    # Normalize filters\n",
        "    f_min, f_max = filters.min(), filters.max()\n",
        "    filters_normalized = (filters - f_min) / (f_max - f_min)\n",
        "\n",
        "    # Plot first 32 filters\n",
        "    n_filters = min(32, filters.shape[-1])\n",
        "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(n_filters):\n",
        "        axes[i].imshow(filters_normalized[:, :, 0, i], cmap='viridis')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'F{i+1}', fontsize=8)\n",
        "\n",
        "    plt.suptitle('CNN First Layer Filters (32 filters)', size=14, weight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Feature importance termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6AV86MHNI5"
      },
      "source": [
        "## üî¨ Section 16: Interpretability (GradCAM, LIME, SHAP)\n",
        "\n",
        "Techniques d'interpr√©tabilit√© pour comprendre les d√©cisions des mod√®les."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkqTnzLSHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INTERPRETABILITY - GRADCAM\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"INTERPRETABILITY - GRADCAM\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from src.interpretability.gradcam import GradCAM\n",
        "\n",
        "# Select a few test images\n",
        "sample_indices = np.random.choice(len(X_test), 8, replace=False)\n",
        "sample_images = X_test_rgb[sample_indices]\n",
        "sample_labels = y_test[sample_indices]\n",
        "\n",
        "# GradCAM for best Transfer Learning model\n",
        "if transfer_results and best_tl:\n",
        "    print(f\"\\nüì∏ GradCAM pour {best_tl_key}\")\n",
        "\n",
        "    gradcam = GradCAM(model=best_tl['model'])\n",
        "\n",
        "    fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
        "\n",
        "    for i, (img, true_label) in enumerate(zip(sample_images, sample_labels)):\n",
        "        # Original image\n",
        "        axes[0, i].imshow(img[:, :, 0], cmap='gray')\n",
        "        axes[0, i].set_title(f'True: {config.classes[true_label]}', fontsize=9)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # GradCAM heatmap\n",
        "        img_expanded = np.expand_dims(img, axis=0)\n",
        "        heatmap = gradcam.compute_heatmap(img_expanded, class_idx=true_label)\n",
        "\n",
        "        axes[1, i].imshow(img[:, :, 0], cmap='gray')\n",
        "        axes[1, i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "        axes[1, i].set_title('GradCAM', fontsize=9)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(f'GradCAM Visualization - {best_tl_key}', size=14, weight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# LIME Interpretability\n",
        "if LIME_AVAILABLE:\n",
        "    print(\"\\nüî¨ LIME Interpretability\")\n",
        "    print(\"   ‚úÖ LIME disponible - visualisations possibles\")\n",
        "    print(\"   (Skipped for efficiency - ajouter si n√©cessaire)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è LIME non disponible\")\n",
        "\n",
        "# SHAP Values\n",
        "if SHAP_AVAILABLE:\n",
        "    print(\"\\nüìä SHAP Values\")\n",
        "    print(\"   ‚úÖ SHAP disponible - analyses possibles\")\n",
        "    print(\"   (Skipped for efficiency - ajouter si n√©cessaire)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è SHAP non disponible\")\n",
        "\n",
        "print(\"\\n‚úÖ Interpretability termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScVZ7_9vHNI5"
      },
      "source": [
        "## ‚ùå Section 17: Error Analysis\n",
        "\n",
        "Analyse des erreurs pour identifier les cas difficiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z_-dmDuHNI5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ERROR ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Analyze errors for best model\n",
        "if transfer_results:\n",
        "    y_pred_best = best_tl['y_pred']\n",
        "    model_name_best = best_tl_key\n",
        "else:\n",
        "    y_pred_best = best_cnn['y_pred']\n",
        "    model_name_best = best_cnn_key\n",
        "\n",
        "# Find misclassified samples\n",
        "errors = y_test != y_pred_best\n",
        "error_indices = np.where(errors)[0]\n",
        "\n",
        "print(f\"\\nüìä Error Statistics pour {model_name_best}:\")\n",
        "print(f\"   Total errors: {errors.sum()}/{len(y_test)} ({errors.sum()/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "# Analyze error patterns\n",
        "error_matrix = np.zeros((config.num_classes, config.num_classes), dtype=int)\n",
        "for true_label, pred_label in zip(y_test[errors], y_pred_best[errors]):\n",
        "    error_matrix[true_label, pred_label] += 1\n",
        "\n",
        "print(\"\\nüìã Error Matrix (True ‚Üí Predicted):\")\n",
        "print(f\"{'':>15s}\", end=\"\")\n",
        "for cls in config.classes:\n",
        "    print(f\"{cls:>15s}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, cls in enumerate(config.classes):\n",
        "    print(f\"{cls:>15s}\", end=\"\")\n",
        "    for j in range(len(config.classes)):\n",
        "        print(f\"{error_matrix[i, j]:>15d}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "# Visualize worst predictions\n",
        "print(\"\\nüì∏ Top 20 Misclassified Images\")\n",
        "\n",
        "# Get prediction probabilities (if available)\n",
        "n_errors_to_show = min(20, len(error_indices))\n",
        "sample_error_indices = error_indices[:n_errors_to_show]\n",
        "\n",
        "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, err_idx in enumerate(sample_error_indices):\n",
        "    axes[idx].imshow(X_test[err_idx], cmap='gray')\n",
        "    true_cls = config.classes[y_test[err_idx]]\n",
        "    pred_cls = config.classes[y_pred_best[err_idx]]\n",
        "    axes[idx].set_title(f'True: {true_cls}\\nPred: {pred_cls}', fontsize=8)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Top 20 Misclassified Samples', size=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Error analysis termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wBZaMwsHNI5"
      },
      "source": [
        "## üìâ Section 18: ROC & PR Curves\n",
        "\n",
        "Courbes ROC et Precision-Recall pour √©valuation multi-classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1yCBQBXHNI6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ROC & PRECISION-RECALL CURVES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ROC & PRECISION-RECALL CURVES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "# Binarize labels for multi-class ROC\n",
        "y_test_bin = label_binarize(y_test, classes=range(config.num_classes))\n",
        "\n",
        "# Get prediction probabilities\n",
        "if transfer_results:\n",
        "    best_model = best_tl['model']\n",
        "    y_pred_proba = best_model.predict(X_test_rgb)\n",
        "    model_name = best_tl_key\n",
        "else:\n",
        "    best_model = best_cnn['model']\n",
        "    y_pred_proba = best_model.predict(X_test_cnn)\n",
        "    model_name = best_cnn_key\n",
        "\n",
        "# ROC Curves\n",
        "print(f\"\\nüìä ROC Curves - {model_name}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ROC for each class\n",
        "ax = axes[0]\n",
        "for i, cls_name in enumerate(config.classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, linewidth=2, label=f'{cls_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curves (One-vs-Rest)', fontsize=12, weight='bold')\n",
        "ax.legend(loc='lower right', fontsize=9)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curves\n",
        "ax = axes[1]\n",
        "for i, cls_name in enumerate(config.classes):\n",
        "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "    avg_precision = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "    ax.plot(recall, precision, linewidth=2, label=f'{cls_name} (AP = {avg_precision:.3f})')\n",
        "\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision-Recall Curves', fontsize=12, weight='bold')\n",
        "ax.legend(loc='lower left', fontsize=9)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ ROC & PR curves g√©n√©r√©es\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxjaeU64HNI6"
      },
      "source": [
        "## üíæ Section 19: Model Saving\n",
        "\n",
        "Sauvegarde des mod√®les entra√Æn√©s et des m√©tadonn√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK3tYVCnHNI6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL SAVING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL SAVING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create models directory\n",
        "models_save_dir = config.models_dir / 'comprehensive_ml_pipeline'\n",
        "models_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÇ Save directory: {models_save_dir}\")\n",
        "\n",
        "# Save best ML model\n",
        "ml_save_path = models_save_dir / f'best_ml_model_{best_ml_key.replace(\" \", \"_\")}.pkl'\n",
        "with open(ml_save_path, 'wb') as f:\n",
        "    pickle.dump(best_ml['model'], f)\n",
        "print(f\"‚úÖ Saved ML model: {ml_save_path.name}\")\n",
        "\n",
        "# Save best CNN model\n",
        "cnn_save_path = models_save_dir / f'best_cnn_model_{best_cnn_key}.keras'\n",
        "best_cnn['model'].save(cnn_save_path)\n",
        "print(f\"‚úÖ Saved CNN model: {cnn_save_path.name}\")\n",
        "\n",
        "# Save best Transfer Learning model\n",
        "if transfer_results:\n",
        "    tl_save_path = models_save_dir / f'best_tl_model_{best_tl_key}.keras'\n",
        "    best_tl['model'].save(tl_save_path)\n",
        "    print(f\"‚úÖ Saved TL model: {tl_save_path.name}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'dataset_info': {\n",
        "        'n_images_per_class': N_IMAGES_PER_CLASS,\n",
        "        'total_images': len(images),\n",
        "        'classes': config.classes,\n",
        "        'img_size': config.img_size\n",
        "    },\n",
        "    'best_models': {\n",
        "        'ml': {\n",
        "            'name': best_ml_key,\n",
        "            'test_acc': float(best_ml['test_acc']),\n",
        "            'f1_weighted': float(best_ml['f1_weighted'])\n",
        "        },\n",
        "        'cnn': {\n",
        "            'name': best_cnn_key,\n",
        "            'test_acc': float(best_cnn['test_acc']),\n",
        "            'f1_weighted': float(best_cnn['f1_weighted'])\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "if transfer_results:\n",
        "    metadata['best_models']['transfer_learning'] = {\n",
        "        'name': best_tl_key,\n",
        "        'test_acc': float(best_tl['test_acc']),\n",
        "        'f1_weighted': float(best_tl['f1_weighted'])\n",
        "    }\n",
        "\n",
        "metadata_path = models_save_dir / 'metadata.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json_lib.dump(metadata, f, indent=2)\n",
        "print(f\"‚úÖ Saved metadata: {metadata_path.name}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model saving termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6UgfwqTHNI6"
      },
      "source": [
        "## üîÆ Section 20: Prediction Pipeline\n",
        "\n",
        "Pipeline de pr√©diction pr√™t pour la production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyRwsXVBHNI6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREDICTION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PREDICTION PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def predict_covid(image_path, model, preprocessing_func=None):\n",
        "    \"\"\"\n",
        "    Pipeline de pr√©diction complet pour une nouvelle image.\n",
        "\n",
        "    Args:\n",
        "        image_path: Chemin vers l'image\n",
        "        model: Mod√®le entra√Æn√©\n",
        "        preprocessing_func: Fonction de preprocessing optionnelle\n",
        "\n",
        "    Returns:\n",
        "        dict avec pr√©diction, probabilit√©s et classe\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    from PIL import Image\n",
        "    img = Image.open(image_path).convert('L')  # Grayscale\n",
        "    img = img.resize(config.img_size)\n",
        "    img_array = np.array(img) / 255.0\n",
        "\n",
        "    # Prepare for model\n",
        "    if preprocessing_func:\n",
        "        img_array = preprocessing_func(img_array)\n",
        "\n",
        "    # Add batch and channel dimensions\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    if len(img_array.shape) == 3:\n",
        "        img_array = np.expand_dims(img_array, axis=-1)\n",
        "\n",
        "    # For transfer learning models, convert to RGB\n",
        "    if img_array.shape[-1] == 1:\n",
        "        img_array = np.repeat(img_array, 3, axis=-1)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(img_array, verbose=1)\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = config.classes[predicted_class_idx]\n",
        "    confidence = predictions[0][predicted_class_idx]\n",
        "\n",
        "    return {\n",
        "        'predicted_class': predicted_class,\n",
        "        'predicted_class_idx': predicted_class_idx,\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {\n",
        "            config.classes[i]: float(predictions[0][i])\n",
        "            for i in range(len(config.classes))\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Test prediction pipeline\n",
        "print(\"\\nüß™ Test du pipeline de pr√©diction\")\n",
        "\n",
        "test_img_path = image_paths[0]  # Premier √©chantillon\n",
        "result = predict_covid(test_img_path, best_tl['model'] if transfer_results else best_cnn['model'])\n",
        "\n",
        "print(f\"\\nTest image: {test_img_path.name}\")\n",
        "print(f\"Predicted class: {result['predicted_class']}\")\n",
        "print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "print(\"\\nAll probabilities:\")\n",
        "for cls, prob in result['all_probabilities'].items():\n",
        "    print(f\"   {cls:20s}: {prob:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Prediction pipeline pr√™t\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nbSpwx7HNI7"
      },
      "source": [
        "## üèÜ Section 21: Performance Benchmarking\n",
        "\n",
        "Comparaison finale de tous les mod√®les entra√Æn√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er0Qc5hyHNI7"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PERFORMANCE BENCHMARKING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PERFORMANCE BENCHMARKING - TABLEAU R√âCAPITULATIF\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Collect all results\n",
        "all_results = []\n",
        "\n",
        "# ML Models (top 10)\n",
        "for model_name, results in sorted(ml_results.items(), key=lambda x: x[1]['test_acc'], reverse=True)[:10]:\n",
        "    all_results.append({\n",
        "        'Model': model_name,\n",
        "        'Type': 'ML',\n",
        "        'Test Acc': results['test_acc'],\n",
        "        'F1 Score': results['f1_weighted'],\n",
        "        'Train Time (s)': results['train_time'],\n",
        "        'Inference (ms)': results['inference_time_ms']\n",
        "    })\n",
        "\n",
        "# CNN Models\n",
        "for model_name, results in cnn_results.items():\n",
        "    all_results.append({\n",
        "        'Model': model_name,\n",
        "        'Type': 'CNN',\n",
        "        'Test Acc': results['test_acc'],\n",
        "        'F1 Score': results['f1_weighted'],\n",
        "        'Train Time (s)': results['train_time'],\n",
        "        'Inference (ms)': 0  # Not measured for CNNs\n",
        "    })\n",
        "\n",
        "# Transfer Learning Models (top 10)\n",
        "if transfer_results:\n",
        "    for model_name, results in sorted(transfer_results.items(), key=lambda x: x[1]['test_acc'], reverse=True)[:10]:\n",
        "        all_results.append({\n",
        "            'Model': model_name,\n",
        "            'Type': 'Transfer Learning',\n",
        "            'Test Acc': results['test_acc'],\n",
        "            'F1 Score': results['f1_weighted'],\n",
        "            'Train Time (s)': results['train_time'],\n",
        "            'Inference (ms)': 0\n",
        "        })\n",
        "\n",
        "# Ensemble Models\n",
        "for model_name, results in ensemble_results.items():\n",
        "    all_results.append({\n",
        "        'Model': f'Ensemble_{model_name}',\n",
        "        'Type': 'Ensemble',\n",
        "        'Test Acc': results['acc'],\n",
        "        'F1 Score': results['f1'],\n",
        "        'Train Time (s)': 0,\n",
        "        'Inference (ms)': 0\n",
        "    })\n",
        "\n",
        "# Sort by accuracy\n",
        "all_results_sorted = sorted(all_results, key=lambda x: x['Test Acc'], reverse=True)\n",
        "\n",
        "# Display table\n",
        "print(\"\\nüìä TOP 20 MODELS:\\n\")\n",
        "print(f\"{'Rank':<6s} {'Model':<40s} {'Type':<20s} {'Test Acc':>10s} {'F1 Score':>10s} {'Train Time':>12s}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for rank, result in enumerate(all_results_sorted[:20], 1):\n",
        "    print(f\"{rank:<6d} {result['Model']:<40s} {result['Type']:<20s} \"\n",
        "          f\"{result['Test Acc']:>10.4f} {result['F1 Score']:>10.4f} \"\n",
        "          f\"{result['Train Time (s)']:>12.1f}\")\n",
        "\n",
        "# Best model overall\n",
        "best_overall = all_results_sorted[0]\n",
        "print(f\"\\nüèÜ BEST MODEL OVERALL:\")\n",
        "print(f\"   {best_overall['Model']}\")\n",
        "print(f\"   Test Accuracy: {best_overall['Test Acc']:.4f}\")\n",
        "print(f\"   F1 Score: {best_overall['F1 Score']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Performance benchmarking termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9fiYMK3HNI7"
      },
      "source": [
        "## üìÑ Section 22: Rapport HTML Automatique\n",
        "\n",
        "G√©n√©ration d'un rapport HTML professionnel avec tous les r√©sultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJNxp6FbHNI7"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RAPPORT HTML AUTOMATIQUE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"G√âN√âRATION DU RAPPORT HTML\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "report_dir = config.results_dir / 'comprehensive_ml_pipeline'\n",
        "report_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "html_report_path = report_dir / 'comprehensive_ml_report.html'\n",
        "\n",
        "# Generate HTML content\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"fr\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Comprehensive ML Pipeline Report - COVID-19 Classification</title>\n",
        "    <style>\n",
        "        body {{\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "            max-width: 1200px;\n",
        "            margin: 0 auto;\n",
        "            padding: 20px;\n",
        "            background-color: #f5f5f5;\n",
        "        }}\n",
        "        h1 {{\n",
        "            color: #2c3e50;\n",
        "            border-bottom: 3px solid #3498db;\n",
        "            padding-bottom: 10px;\n",
        "        }}\n",
        "        h2 {{\n",
        "            color: #34495e;\n",
        "            margin-top: 30px;\n",
        "        }}\n",
        "        .metric-box {{\n",
        "            background-color: white;\n",
        "            padding: 20px;\n",
        "            margin: 15px 0;\n",
        "            border-radius: 8px;\n",
        "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "        }}\n",
        "        .best-model {{\n",
        "            background-color: #d4edda;\n",
        "            border-left: 4px solid #28a745;\n",
        "        }}\n",
        "        table {{\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin: 20px 0;\n",
        "            background-color: white;\n",
        "        }}\n",
        "        th, td {{\n",
        "            padding: 12px;\n",
        "            text-align: left;\n",
        "            border-bottom: 1px solid #ddd;\n",
        "        }}\n",
        "        th {{\n",
        "            background-color: #3498db;\n",
        "            color: white;\n",
        "        }}\n",
        "        .footer {{\n",
        "            margin-top: 40px;\n",
        "            text-align: center;\n",
        "            color: #7f8c8d;\n",
        "            font-size: 0.9em;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>ü¶† Comprehensive ML Pipeline Report</h1>\n",
        "    <p><strong>Date:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "\n",
        "    <h2>üìä Dataset Information</h2>\n",
        "    <div class=\"metric-box\">\n",
        "        <p><strong>Total Images:</strong> {len(images)}</p>\n",
        "        <p><strong>Classes:</strong> {', '.join(config.classes)}</p>\n",
        "        <p><strong>Train/Val/Test Split:</strong> {len(X_train)}/{len(X_val)}/{len(X_test)}</p>\n",
        "    </div>\n",
        "\n",
        "    <h2>üèÜ Best Models</h2>\n",
        "\n",
        "    <div class=\"metric-box best-model\">\n",
        "        <h3>Best ML Model: {best_ml_key}</h3>\n",
        "        <p><strong>Test Accuracy:</strong> {best_ml['test_acc']:.4f}</p>\n",
        "        <p><strong>F1 Score:</strong> {best_ml['f1_weighted']:.4f}</p>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"metric-box best-model\">\n",
        "        <h3>Best CNN Model: {best_cnn_key}</h3>\n",
        "        <p><strong>Test Accuracy:</strong> {best_cnn['test_acc']:.4f}</p>\n",
        "        <p><strong>F1 Score:</strong> {best_cnn['f1_weighted']:.4f}</p>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "if transfer_results:\n",
        "    html_content += f\"\"\"\n",
        "    <div class=\"metric-box best-model\">\n",
        "        <h3>Best Transfer Learning Model: {best_tl_key}</h3>\n",
        "        <p><strong>Test Accuracy:</strong> {best_tl['test_acc']:.4f}</p>\n",
        "        <p><strong>F1 Score:</strong> {best_tl['f1_weighted']:.4f}</p>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "html_content += \"\"\"\n",
        "    <h2>üìà Top 20 Models Ranking</h2>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Rank</th>\n",
        "            <th>Model</th>\n",
        "            <th>Type</th>\n",
        "            <th>Test Accuracy</th>\n",
        "            <th>F1 Score</th>\n",
        "        </tr>\n",
        "\"\"\"\n",
        "\n",
        "for rank, result in enumerate(all_results_sorted[:20], 1):\n",
        "    html_content += f\"\"\"\n",
        "        <tr>\n",
        "            <td>{rank}</td>\n",
        "            <td>{result['Model']}</td>\n",
        "            <td>{result['Type']}</td>\n",
        "            <td>{result['Test Acc']:.4f}</td>\n",
        "            <td>{result['F1 Score']:.4f}</td>\n",
        "        </tr>\n",
        "\"\"\"\n",
        "\n",
        "html_content += \"\"\"\n",
        "    </table>\n",
        "\n",
        "    <div class=\"footer\">\n",
        "        <p>Generated by Comprehensive ML Pipeline Notebook</p>\n",
        "        <p>COVID-19 Radiography Classification Project</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Save HTML report\n",
        "with open(html_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"\\n‚úÖ Rapport HTML g√©n√©r√©: {html_report_path}\")\n",
        "print(f\"   Ouvrir dans un navigateur pour visualiser\")\n",
        "\n",
        "print(\"\\n‚úÖ Rapport HTML termin√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AIZffvFHNI7"
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### ‚úÖ R√©alisations\n",
        "\n",
        "Ce notebook a accompli:\n",
        "\n",
        "1. **27 mod√®les ML classiques** (9 algorithmes √ó 3 feature sets)\n",
        "2. **3 architectures CNN** personnalis√©es\n",
        "3. **12 mod√®les Transfer Learning** avec fine-tuning 2-phase\n",
        "4. **M√©thodes d'ensemble** (Voting, Stacking)\n",
        "5. **Hyperparameter Optimization** (GridSearchCV, Optuna)\n",
        "6. **K-Fold Cross-Validation** (5-fold stratifi√©)\n",
        "7. **Interpr√©tabilit√©** (GradCAM, LIME, SHAP)\n",
        "8. **Analysis compl√®te** (Learning curves, Confusion matrices, ROC curves, Error analysis)\n",
        "9. **Production-ready** (Model saving, Prediction pipeline, HTML report)\n",
        "\n",
        "### üèÜ Meilleur Mod√®le\n",
        "\n",
        "Le meilleur mod√®le a √©t√© identifi√© et sauvegard√© avec ses m√©tadonn√©es.\n",
        "\n",
        "### üìö Prochaines √âtapes\n",
        "\n",
        "1. **D√©ploiement**: Int√©grer le mod√®le dans une application web ou API\n",
        "2. **Monitoring**: Mettre en place un syst√®me de surveillance des performances\n",
        "3. **Am√©lioration continue**: Collecter plus de donn√©es et r√©entra√Æner r√©guli√®rement\n",
        "\n",
        "### üìû Support\n",
        "\n",
        "Pour toute question ou am√©lioration, r√©f√©rez-vous au repository GitHub:\n",
        "https://github.com/L-Poca/Data_Pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**Merci d'avoir utilis√© ce notebook! üôè**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Notebook_end_time = time.time()\n",
        "Notebook_duration = Notebook_end_time - Notebook_begin_time\n",
        "\n",
        "# passer le temps au format heures, minutes, secondes\n",
        "hours, rem = divmod(Notebook_duration, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"\\n‚è±Ô∏è Temps total d'ex√©cution du notebook: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
