# üîç Modules d'Interpr√©tabilit√© pour Deep Learning

Modules complets pour comprendre les d√©cisions des mod√®les CNN appliqu√©s √† la classification COVID-19.

## üì¶ Installation

```bash
pip install lime shap scikit-image
```

## üéØ Modules Disponibles

### 1. **GradCAM** (`gradcam.py`)
Visualise les zones d'attention du mod√®le via les gradients des couches convolutionnelles.

**Classes:**
- `GradCAM`: Calcul des heatmaps Grad-CAM
- `visualize_gradcam()`: Visualisation simple
- `visualize_gradcam_grid()`: Grille de visualisations
- `compare_layers()`: Comparaison entre couches
- `overlay_heatmap()`: Superposition heatmap/image

**Exemple:**
```python
from src.interpretability import GradCAM, visualize_gradcam

# Cr√©er l'explainer
gradcam = GradCAM(model, layer_name='block5_conv3')

# Calculer la heatmap
heatmap = gradcam.compute_heatmap(image, class_idx=0)

# Visualiser
visualize_gradcam(image, heatmap, class_name='COVID', confidence=0.95)
```

### 2. **LIME** (`lime_explainer.py`)
Explications par segmentation d'image (super-pixels).

**Classes:**
- `LIMEImageExplainer`: Explainer LIME pour images
- `quick_lime_explanation()`: Fonction rapide

**M√©thodes de segmentation:**
- `quickshift`: Rapide, bon √©quilibre
- `felzenszwalb`: Segmentation bas√©e sur les graphes
- `slic`: Simple Linear Iterative Clustering

**Exemple:**
```python
from src.interpretability import LIMEImageExplainer

# Cr√©er l'explainer
explainer = LIMEImageExplainer(model.predict, num_samples=1000)

# G√©n√©rer l'explication
explanation = explainer.explain_instance(image, top_labels=1, num_features=10)

# Visualiser
explainer.visualize_explanation(image, explanation, label=0, num_features=5)
```

### 3. **SHAP** (`shap_explainer.py`)
Valeurs de Shapley pour explications au niveau pixel.

**Classes:**
- `SHAPExplainer`: DeepExplainer pour TensorFlow/Keras
- `quick_shap_explanation()`: Fonction rapide

**Visualisations:**
- Image plot (magnitude + signed)
- Heatmap overlay
- Summary plot
- Decision plot
- Comparaison entre classes

**Exemple:**
```python
from src.interpretability import SHAPExplainer

# Cr√©er l'explainer (n√©cessite background data)
explainer = SHAPExplainer(model, background_data)

# Calculer les valeurs SHAP
shap_values = explainer.explain(images)

# Visualiser
explainer.visualize_image_plot(image, shap_values[0], class_idx=0)
```

### 4. **Utilitaires** (`utils.py`)
Fonctions communes pour comparer et analyser les explications.

**Fonctions principales:**
- `plot_multiple_explanations()`: Compare les 3 m√©thodes
- `create_interpretation_report()`: Rapport complet
- `batch_explain()`: Explications en batch
- `compute_explanation_metrics()`: M√©triques d'√©valuation
- `save_explanation()` / `load_explanation()`: Sauvegarde/Chargement

**Exemple:**
```python
from src.interpretability import plot_multiple_explanations

# Comparer les 3 m√©thodes c√¥te √† c√¥te
fig = plot_multiple_explanations(
    image,
    gradcam_heatmap=heatmap,
    lime_explanation=lime_exp,
    shap_values=shap_vals,
    class_idx=0,
    class_name='COVID',
    confidence=0.95
)
```

## üöÄ Usage Rapide

### Option 1: Grad-CAM (Recommand√© pour la rapidit√©)

```python
from src.interpretability import GradCAM, visualize_gradcam

gradcam = GradCAM(model)
heatmap = gradcam.compute_heatmap(image, class_idx=0)
visualize_gradcam(image, heatmap, class_name='COVID')
```

### Option 2: LIME (Bon √©quilibre)

```python
from src.interpretability import LIMEImageExplainer

explainer = LIMEImageExplainer(model.predict)
explanation = explainer.explain_instance(image, top_labels=1)
explainer.visualize_explanation(image, explanation, label=0)
```

### Option 3: SHAP (Le plus rigoureux)

```python
from src.interpretability import SHAPExplainer

explainer = SHAPExplainer(model, background_data)
shap_values = explainer.explain(image[np.newaxis, ...])
explainer.visualize_image_plot(image, shap_values[0], class_idx=0)
```

### Option 4: Rapport Complet

```python
from src.interpretability import create_interpretation_report

report = create_interpretation_report(
    image,
    model,
    class_names=['COVID', 'Normal', 'Lung_Opacity', 'Viral Pneumonia'],
    true_label=0,
    pred_label=0,
    confidence=0.95,
    save_dir=Path('./results'),
    background_data=X_train[:100]  # Pour SHAP
)
```

## üìä Comparaison des M√©thodes

| Crit√®re | Grad-CAM | LIME | SHAP |
|---------|----------|------|------|
| **Vitesse** | ‚ö°‚ö°‚ö° Tr√®s rapide | ‚ö°‚ö° Moyen | ‚ö° Lent |
| **Pr√©cision** | ‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê Excellente |
| **Interpr√©tabilit√©** | ‚≠ê‚≠ê‚≠ê Intuitive | ‚≠ê‚≠ê‚≠ê Tr√®s bonne | ‚≠ê‚≠ê Complexe |
| **Model-agnostic** | ‚ùå CNN uniquement | ‚úÖ Oui | ‚úÖ Oui |
| **Background data** | ‚ùå Non requis | ‚ùå Non requis | ‚úÖ Requis |
| **Batch processing** | ‚úÖ Excellent | ‚ö†Ô∏è Possible | ‚ö†Ô∏è Lent |

## üí° Recommandations d'Usage

### Pour la Production
- **Grad-CAM**: Rapide, efficace, suffisant pour la majorit√© des cas

### Pour l'Analyse Exploratoire
- **Grad-CAM + LIME**: Combiner vision globale et locale

### Pour la Recherche
- **SHAP**: Explications th√©oriquement fond√©es

### Pour Communication M√©dicale
- **Grad-CAM**: Plus visuel et facile √† comprendre

## üîß Configuration Avanc√©e

### Grad-CAM - Choix de la couche

```python
# Lister les couches disponibles
gradcam = GradCAM(model)
layers = gradcam.get_available_layers()
print(layers)

# Utiliser une couche sp√©cifique
gradcam = GradCAM(model, layer_name='block4_conv3')
```

### LIME - M√©thodes de segmentation

```python
# Comparer les m√©thodes
explainer.compare_segmentation_methods(
    image,
    methods=['quickshift', 'felzenszwalb', 'slic']
)

# Choisir la meilleure pour vos donn√©es
explainer = LIMEImageExplainer(
    model.predict,
    segmentation_method='slic',  # Meilleure segmentation
    num_samples=2000  # Plus d'√©chantillons = meilleure pr√©cision
)
```

### SHAP - Optimisation

```python
# Utiliser moins de background data pour acc√©l√©rer
background_subset = X_train[:50]  # 50 images suffisent souvent

# D√©sactiver la v√©rification d'additivit√© (plus rapide)
shap_values = explainer.explain(images, check_additivity=False)
```

## üìà M√©triques d'√âvaluation

√âvaluez la qualit√© des explications:

```python
from src.interpretability.utils import (
    compute_explanation_metrics,
    visualize_metrics_comparison
)

# Calculer les m√©triques
metrics = compare_explanation_metrics(
    gradcam_heatmap=heatmap,
    lime_mask=mask,
    shap_heatmap=shap_map,
    image=image
)

# M√©triques disponibles:
# - coverage: Pourcentage de l'image couvert
# - mean_intensity: Intensit√© moyenne
# - max_intensity: Intensit√© maximale
# - concentration: Degr√© de concentration (entropie inverse)
```

## üìö R√©f√©rences

- **Grad-CAM**: [Selvaraju et al., 2017](https://arxiv.org/abs/1610.02391)
- **LIME**: [Ribeiro et al., 2016](https://arxiv.org/abs/1602.04938)
- **SHAP**: [Lundberg & Lee, 2017](https://arxiv.org/abs/1705.07874)

## üêõ Troubleshooting

### Erreur: "LIME non install√©"
```bash
pip install lime
```

### Erreur: "SHAP non install√©"
```bash
pip install shap
```

### SHAP trop lent
- R√©duire le nombre d'images background (50 suffit)
- D√©sactiver `check_additivity=False`
- Utiliser Grad-CAM √† la place

### Grad-CAM: Couche non trouv√©e
```python
# Lister les couches disponibles
gradcam = GradCAM(model)
print(gradcam.get_available_layers())
```

## üìñ Documentation Compl√®te

Voir le notebook de d√©monstration: `notebooks/interpretability_demo.ipynb`

## ‚úÖ Tests

```python
# Test rapide
from src.interpretability import GradCAM

model = keras.models.load_model('models/vgg16_finetuned_best.keras')
gradcam = GradCAM(model)
print("‚úÖ Modules d'interpr√©tabilit√© op√©rationnels")
```

## üéØ Prochaines √âtapes

1. Int√©grer dans le pipeline de production
2. Cr√©er des dashboards interactifs (Streamlit/Dash)
3. Automatiser l'analyse des erreurs
4. G√©n√©rer des rapports PDF pour les m√©decins
